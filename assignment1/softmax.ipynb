{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T14:23:41.421863+03:00",
     "start_time": "2017-12-08T14:23:40.975648Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T14:23:49.464548+03:00",
     "start_time": "2017-12-08T14:23:41.423139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T14:23:49.669745+03:00",
     "start_time": "2017-12-08T14:23:49.465927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.317629\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Because we assume that all classes are equally distributed and equation under the log will close to 1/10 = 0.1*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T14:23:55.865650+03:00",
     "start_time": "2017-12-08T14:23:49.671618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -3.190581 analytic: -3.190581, relative error: 9.240005e-09\n",
      "numerical: -3.214506 analytic: -3.214506, relative error: 8.082198e-09\n",
      "numerical: 0.037820 analytic: 0.037820, relative error: 4.044042e-07\n",
      "numerical: 2.379752 analytic: 2.379752, relative error: 1.199829e-08\n",
      "numerical: 2.395646 analytic: 2.395646, relative error: 2.541865e-09\n",
      "numerical: -5.132501 analytic: -5.132501, relative error: 1.317823e-08\n",
      "numerical: 0.744267 analytic: 0.744267, relative error: 2.530428e-08\n",
      "numerical: 0.429383 analytic: 0.429383, relative error: 6.991284e-08\n",
      "numerical: -0.436345 analytic: -0.436345, relative error: 1.458362e-09\n",
      "numerical: 1.059291 analytic: 1.059291, relative error: 9.126426e-08\n",
      "numerical: -0.058604 analytic: -0.060209, relative error: 1.350986e-02\n",
      "numerical: -0.653015 analytic: -0.647506, relative error: 4.235902e-03\n",
      "numerical: 0.311931 analytic: 0.304397, relative error: 1.222351e-02\n",
      "numerical: 1.014163 analytic: 1.017654, relative error: 1.718015e-03\n",
      "numerical: 0.156081 analytic: 0.148332, relative error: 2.545263e-02\n",
      "numerical: -1.082420 analytic: -1.084563, relative error: 9.890280e-04\n",
      "numerical: 1.408663 analytic: 1.413791, relative error: 1.816769e-03\n",
      "numerical: 4.076392 analytic: 4.075036, relative error: 1.663611e-04\n",
      "numerical: 1.550853 analytic: 1.555671, relative error: 1.551128e-03\n",
      "numerical: 0.849871 analytic: 0.856507, relative error: 3.888591e-03\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T14:23:56.043926+03:00",
     "start_time": "2017-12-08T14:23:55.867208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.317629e+00 computed in 0.140432s\n",
      "vectorized loss: 2.317629e+00 computed in 0.014379s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T14:56:06.022528+03:00",
     "start_time": "2017-12-08T14:23:56.050179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-09 100.0\n",
      "iteration 0 / 1500: loss 8.663401\n",
      "iteration 100 / 1500: loss 9.074318\n",
      "iteration 200 / 1500: loss 8.492918\n",
      "iteration 300 / 1500: loss 8.665136\n",
      "iteration 400 / 1500: loss 8.567482\n",
      "iteration 500 / 1500: loss 8.770124\n",
      "iteration 600 / 1500: loss 8.725410\n",
      "iteration 700 / 1500: loss 8.653277\n",
      "iteration 800 / 1500: loss 8.503920\n",
      "iteration 900 / 1500: loss 8.571812\n",
      "iteration 1000 / 1500: loss 8.249680\n",
      "iteration 1100 / 1500: loss 8.483242\n",
      "iteration 1200 / 1500: loss 8.135791\n",
      "iteration 1300 / 1500: loss 8.341938\n",
      "iteration 1400 / 1500: loss 8.911495\n",
      "1e-09 215.443469003\n",
      "iteration 0 / 1500: loss 12.007899\n",
      "iteration 100 / 1500: loss 12.000319\n",
      "iteration 200 / 1500: loss 12.123497\n",
      "iteration 300 / 1500: loss 11.748989\n",
      "iteration 400 / 1500: loss 11.847371\n",
      "iteration 500 / 1500: loss 11.781560\n",
      "iteration 600 / 1500: loss 11.886037\n",
      "iteration 700 / 1500: loss 11.509335\n",
      "iteration 800 / 1500: loss 12.092792\n",
      "iteration 900 / 1500: loss 11.427069\n",
      "iteration 1000 / 1500: loss 11.889408\n",
      "iteration 1100 / 1500: loss 11.521820\n",
      "iteration 1200 / 1500: loss 11.766433\n",
      "iteration 1300 / 1500: loss 11.727793\n",
      "iteration 1400 / 1500: loss 11.618256\n",
      "1e-09 464.158883361\n",
      "iteration 0 / 1500: loss 19.683402\n",
      "iteration 100 / 1500: loss 20.094509\n",
      "iteration 200 / 1500: loss 20.235012\n",
      "iteration 300 / 1500: loss 20.182524\n",
      "iteration 400 / 1500: loss 20.270440\n",
      "iteration 500 / 1500: loss 19.707703\n",
      "iteration 600 / 1500: loss 19.932988\n",
      "iteration 700 / 1500: loss 20.429215\n",
      "iteration 800 / 1500: loss 19.674598\n",
      "iteration 900 / 1500: loss 19.599238\n",
      "iteration 1000 / 1500: loss 19.716364\n",
      "iteration 1100 / 1500: loss 20.196022\n",
      "iteration 1200 / 1500: loss 19.969010\n",
      "iteration 1300 / 1500: loss 20.061305\n",
      "iteration 1400 / 1500: loss 19.583109\n",
      "1e-09 1000.0\n",
      "iteration 0 / 1500: loss 36.353921\n",
      "iteration 100 / 1500: loss 36.465976\n",
      "iteration 200 / 1500: loss 36.219857\n",
      "iteration 300 / 1500: loss 35.909241\n",
      "iteration 400 / 1500: loss 36.310588\n",
      "iteration 500 / 1500: loss 36.004512\n",
      "iteration 600 / 1500: loss 35.969650\n",
      "iteration 700 / 1500: loss 35.883517\n",
      "iteration 800 / 1500: loss 36.084153\n",
      "iteration 900 / 1500: loss 35.513437\n",
      "iteration 1000 / 1500: loss 36.196511\n",
      "iteration 1100 / 1500: loss 36.378660\n",
      "iteration 1200 / 1500: loss 36.652376\n",
      "iteration 1300 / 1500: loss 35.505527\n",
      "iteration 1400 / 1500: loss 35.734473\n",
      "1e-09 2154.43469003\n",
      "iteration 0 / 1500: loss 71.249591\n",
      "iteration 100 / 1500: loss 70.799040\n",
      "iteration 200 / 1500: loss 71.121201\n",
      "iteration 300 / 1500: loss 71.174869\n",
      "iteration 400 / 1500: loss 70.772123\n",
      "iteration 500 / 1500: loss 70.862782\n",
      "iteration 600 / 1500: loss 71.136909\n",
      "iteration 700 / 1500: loss 70.764210\n",
      "iteration 800 / 1500: loss 70.675679\n",
      "iteration 900 / 1500: loss 70.790099\n",
      "iteration 1000 / 1500: loss 70.336672\n",
      "iteration 1100 / 1500: loss 70.655634\n",
      "iteration 1200 / 1500: loss 70.610567\n",
      "iteration 1300 / 1500: loss 70.315257\n",
      "iteration 1400 / 1500: loss 70.176359\n",
      "1e-09 4641.58883361\n",
      "iteration 0 / 1500: loss 149.765982\n",
      "iteration 100 / 1500: loss 149.832633\n",
      "iteration 200 / 1500: loss 149.441328\n",
      "iteration 300 / 1500: loss 148.859206\n",
      "iteration 400 / 1500: loss 149.014676\n",
      "iteration 500 / 1500: loss 148.782212\n",
      "iteration 600 / 1500: loss 148.928122\n",
      "iteration 700 / 1500: loss 148.205899\n",
      "iteration 800 / 1500: loss 147.972856\n",
      "iteration 900 / 1500: loss 147.710091\n",
      "iteration 1000 / 1500: loss 147.847997\n",
      "iteration 1100 / 1500: loss 147.348776\n",
      "iteration 1200 / 1500: loss 147.742764\n",
      "iteration 1300 / 1500: loss 147.879293\n",
      "iteration 1400 / 1500: loss 147.277265\n",
      "1e-09 10000.0\n",
      "iteration 0 / 1500: loss 311.631790\n",
      "iteration 100 / 1500: loss 310.405834\n",
      "iteration 200 / 1500: loss 310.232396\n",
      "iteration 300 / 1500: loss 309.254844\n",
      "iteration 400 / 1500: loss 308.976137\n",
      "iteration 500 / 1500: loss 308.121682\n",
      "iteration 600 / 1500: loss 307.240187\n",
      "iteration 700 / 1500: loss 306.825973\n",
      "iteration 800 / 1500: loss 306.444298\n",
      "iteration 900 / 1500: loss 305.464243\n",
      "iteration 1000 / 1500: loss 305.755670\n",
      "iteration 1100 / 1500: loss 304.513587\n",
      "iteration 1200 / 1500: loss 303.612567\n",
      "iteration 1300 / 1500: loss 303.578604\n",
      "iteration 1400 / 1500: loss 302.810120\n",
      "1e-09 21544.3469003\n",
      "iteration 0 / 1500: loss 668.060446\n",
      "iteration 100 / 1500: loss 665.530489\n",
      "iteration 200 / 1500: loss 662.319027\n",
      "iteration 300 / 1500: loss 659.651111\n",
      "iteration 400 / 1500: loss 656.354322\n",
      "iteration 500 / 1500: loss 653.393514\n",
      "iteration 600 / 1500: loss 651.025082\n",
      "iteration 700 / 1500: loss 647.554927\n",
      "iteration 800 / 1500: loss 644.800640\n",
      "iteration 900 / 1500: loss 642.210710\n",
      "iteration 1000 / 1500: loss 639.774922\n",
      "iteration 1100 / 1500: loss 636.948448\n",
      "iteration 1200 / 1500: loss 633.810648\n",
      "iteration 1300 / 1500: loss 631.401436\n",
      "iteration 1400 / 1500: loss 628.563373\n",
      "1e-09 46415.8883361\n",
      "iteration 0 / 1500: loss 1418.819220\n",
      "iteration 100 / 1500: loss 1406.214415\n",
      "iteration 200 / 1500: loss 1392.782522\n",
      "iteration 300 / 1500: loss 1380.065707\n",
      "iteration 400 / 1500: loss 1367.160017\n",
      "iteration 500 / 1500: loss 1354.473332\n",
      "iteration 600 / 1500: loss 1341.563341\n",
      "iteration 700 / 1500: loss 1329.555590\n",
      "iteration 800 / 1500: loss 1317.440267\n",
      "iteration 900 / 1500: loss 1304.702953\n",
      "iteration 1000 / 1500: loss 1292.498365\n",
      "iteration 1100 / 1500: loss 1281.239510\n",
      "iteration 1200 / 1500: loss 1268.949199\n",
      "iteration 1300 / 1500: loss 1257.561181\n",
      "iteration 1400 / 1500: loss 1245.547267\n",
      "1e-09 100000.0\n",
      "iteration 0 / 1500: loss 3105.708613\n",
      "iteration 100 / 1500: loss 3044.143524\n",
      "iteration 200 / 1500: loss 2983.867884\n",
      "iteration 300 / 1500: loss 2924.695059\n",
      "iteration 400 / 1500: loss 2866.667337\n",
      "iteration 500 / 1500: loss 2809.808277\n",
      "iteration 600 / 1500: loss 2754.369654\n",
      "iteration 700 / 1500: loss 2699.978116\n",
      "iteration 800 / 1500: loss 2646.194292\n",
      "iteration 900 / 1500: loss 2593.862472\n",
      "iteration 1000 / 1500: loss 2542.443146\n",
      "iteration 1100 / 1500: loss 2491.608302\n",
      "iteration 1200 / 1500: loss 2442.419232\n",
      "iteration 1300 / 1500: loss 2393.504122\n",
      "iteration 1400 / 1500: loss 2346.629880\n",
      "1e-09 215443.469003\n",
      "iteration 0 / 1500: loss 6638.208714\n",
      "iteration 100 / 1500: loss 6358.457984\n",
      "iteration 200 / 1500: loss 6090.053452\n",
      "iteration 300 / 1500: loss 5833.151220\n",
      "iteration 400 / 1500: loss 5587.304419\n",
      "iteration 500 / 1500: loss 5351.171249\n",
      "iteration 600 / 1500: loss 5125.691526\n",
      "iteration 700 / 1500: loss 4909.184592\n",
      "iteration 800 / 1500: loss 4702.253801\n",
      "iteration 900 / 1500: loss 4503.638206\n",
      "iteration 1000 / 1500: loss 4313.672615\n",
      "iteration 1100 / 1500: loss 4131.813950\n",
      "iteration 1200 / 1500: loss 3957.643009\n",
      "iteration 1300 / 1500: loss 3790.364848\n",
      "iteration 1400 / 1500: loss 3630.523537\n",
      "1e-09 464158.883361\n",
      "iteration 0 / 1500: loss 14274.225178\n",
      "iteration 100 / 1500: loss 13008.503947\n",
      "iteration 200 / 1500: loss 11854.821553\n",
      "iteration 300 / 1500: loss 10803.659605\n",
      "iteration 400 / 1500: loss 9845.619017\n",
      "iteration 500 / 1500: loss 8972.669883\n",
      "iteration 600 / 1500: loss 8177.330099\n",
      "iteration 700 / 1500: loss 7452.061702\n",
      "iteration 800 / 1500: loss 6791.199801\n",
      "iteration 900 / 1500: loss 6188.978520\n",
      "iteration 1000 / 1500: loss 5640.226170\n",
      "iteration 1100 / 1500: loss 5139.980941\n",
      "iteration 1200 / 1500: loss 4684.272904\n",
      "iteration 1300 / 1500: loss 4268.940431\n",
      "iteration 1400 / 1500: loss 3890.589768\n",
      "1e-09 1000000.0\n",
      "iteration 0 / 1500: loss 30889.442785\n",
      "iteration 100 / 1500: loss 25287.191244\n",
      "iteration 200 / 1500: loss 20700.979714\n",
      "iteration 300 / 1500: loss 16946.651773\n",
      "iteration 400 / 1500: loss 13873.398463\n",
      "iteration 500 / 1500: loss 11357.224858\n",
      "iteration 600 / 1500: loss 9297.865389\n",
      "iteration 700 / 1500: loss 7611.791299\n",
      "iteration 800 / 1500: loss 6231.374057\n",
      "iteration 900 / 1500: loss 5101.455131\n",
      "iteration 1000 / 1500: loss 4176.528011\n",
      "iteration 1100 / 1500: loss 3419.350676\n",
      "iteration 1200 / 1500: loss 2799.547421\n",
      "iteration 1300 / 1500: loss 2292.120153\n",
      "iteration 1400 / 1500: loss 1876.649167\n",
      "3.16227766017e-09 100.0\n",
      "iteration 0 / 1500: loss 8.949843\n",
      "iteration 100 / 1500: loss 8.710026\n",
      "iteration 200 / 1500: loss 8.523021\n",
      "iteration 300 / 1500: loss 8.499935\n",
      "iteration 400 / 1500: loss 8.480415\n",
      "iteration 500 / 1500: loss 8.159630\n",
      "iteration 600 / 1500: loss 8.397243\n",
      "iteration 700 / 1500: loss 8.026263\n",
      "iteration 800 / 1500: loss 7.934553\n",
      "iteration 900 / 1500: loss 7.726123\n",
      "iteration 1000 / 1500: loss 7.528292\n",
      "iteration 1100 / 1500: loss 8.059272\n",
      "iteration 1200 / 1500: loss 7.962372\n",
      "iteration 1300 / 1500: loss 7.820090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 1500: loss 7.463674\n",
      "3.16227766017e-09 215.443469003\n",
      "iteration 0 / 1500: loss 12.025769\n",
      "iteration 100 / 1500: loss 12.034300\n",
      "iteration 200 / 1500: loss 12.206860\n",
      "iteration 300 / 1500: loss 11.473561\n",
      "iteration 400 / 1500: loss 11.910030\n",
      "iteration 500 / 1500: loss 12.143746\n",
      "iteration 600 / 1500: loss 11.644297\n",
      "iteration 700 / 1500: loss 11.701268\n",
      "iteration 800 / 1500: loss 11.097462\n",
      "iteration 900 / 1500: loss 12.027999\n",
      "iteration 1000 / 1500: loss 11.139318\n",
      "iteration 1100 / 1500: loss 11.175888\n",
      "iteration 1200 / 1500: loss 11.255591\n",
      "iteration 1300 / 1500: loss 11.097068\n",
      "iteration 1400 / 1500: loss 10.824948\n",
      "3.16227766017e-09 464.158883361\n",
      "iteration 0 / 1500: loss 19.612243\n",
      "iteration 100 / 1500: loss 19.753032\n",
      "iteration 200 / 1500: loss 19.613250\n",
      "iteration 300 / 1500: loss 19.408800\n",
      "iteration 400 / 1500: loss 19.636926\n",
      "iteration 500 / 1500: loss 19.281524\n",
      "iteration 600 / 1500: loss 19.185184\n",
      "iteration 700 / 1500: loss 19.299777\n",
      "iteration 800 / 1500: loss 18.613529\n",
      "iteration 900 / 1500: loss 19.109589\n",
      "iteration 1000 / 1500: loss 18.515670\n",
      "iteration 1100 / 1500: loss 18.537897\n",
      "iteration 1200 / 1500: loss 18.681599\n",
      "iteration 1300 / 1500: loss 18.684641\n",
      "iteration 1400 / 1500: loss 18.696700\n",
      "3.16227766017e-09 1000.0\n",
      "iteration 0 / 1500: loss 36.131256\n",
      "iteration 100 / 1500: loss 36.224872\n",
      "iteration 200 / 1500: loss 36.484337\n",
      "iteration 300 / 1500: loss 36.082825\n",
      "iteration 400 / 1500: loss 35.770520\n",
      "iteration 500 / 1500: loss 35.728980\n",
      "iteration 600 / 1500: loss 35.649920\n",
      "iteration 700 / 1500: loss 35.224368\n",
      "iteration 800 / 1500: loss 35.588996\n",
      "iteration 900 / 1500: loss 35.725657\n",
      "iteration 1000 / 1500: loss 35.892609\n",
      "iteration 1100 / 1500: loss 35.038535\n",
      "iteration 1200 / 1500: loss 34.802032\n",
      "iteration 1300 / 1500: loss 35.291896\n",
      "iteration 1400 / 1500: loss 35.069314\n",
      "3.16227766017e-09 2154.43469003\n",
      "iteration 0 / 1500: loss 71.878886\n",
      "iteration 100 / 1500: loss 71.200592\n",
      "iteration 200 / 1500: loss 71.158428\n",
      "iteration 300 / 1500: loss 70.765528\n",
      "iteration 400 / 1500: loss 70.675636\n",
      "iteration 500 / 1500: loss 70.835145\n",
      "iteration 600 / 1500: loss 70.063764\n",
      "iteration 700 / 1500: loss 70.437869\n",
      "iteration 800 / 1500: loss 70.175044\n",
      "iteration 900 / 1500: loss 69.829023\n",
      "iteration 1000 / 1500: loss 69.931808\n",
      "iteration 1100 / 1500: loss 69.871925\n",
      "iteration 1200 / 1500: loss 69.686590\n",
      "iteration 1300 / 1500: loss 69.322507\n",
      "iteration 1400 / 1500: loss 69.354314\n",
      "3.16227766017e-09 4641.58883361\n",
      "iteration 0 / 1500: loss 148.017568\n",
      "iteration 100 / 1500: loss 147.883932\n",
      "iteration 200 / 1500: loss 147.363226\n",
      "iteration 300 / 1500: loss 146.591867\n",
      "iteration 400 / 1500: loss 146.078212\n",
      "iteration 500 / 1500: loss 145.803839\n",
      "iteration 600 / 1500: loss 145.655012\n",
      "iteration 700 / 1500: loss 144.945296\n",
      "iteration 800 / 1500: loss 144.803734\n",
      "iteration 900 / 1500: loss 143.649485\n",
      "iteration 1000 / 1500: loss 143.113617\n",
      "iteration 1100 / 1500: loss 142.500555\n",
      "iteration 1200 / 1500: loss 142.425111\n",
      "iteration 1300 / 1500: loss 142.245714\n",
      "iteration 1400 / 1500: loss 141.549275\n",
      "3.16227766017e-09 10000.0\n",
      "iteration 0 / 1500: loss 309.931202\n",
      "iteration 100 / 1500: loss 307.745132\n",
      "iteration 200 / 1500: loss 305.613170\n",
      "iteration 300 / 1500: loss 303.752791\n",
      "iteration 400 / 1500: loss 301.597863\n",
      "iteration 500 / 1500: loss 300.012592\n",
      "iteration 600 / 1500: loss 297.700921\n",
      "iteration 700 / 1500: loss 295.984859\n",
      "iteration 800 / 1500: loss 293.924698\n",
      "iteration 900 / 1500: loss 291.747316\n",
      "iteration 1000 / 1500: loss 290.545774\n",
      "iteration 1100 / 1500: loss 287.979526\n",
      "iteration 1200 / 1500: loss 286.394567\n",
      "iteration 1300 / 1500: loss 284.284611\n",
      "iteration 1400 / 1500: loss 282.509692\n",
      "3.16227766017e-09 21544.3469003\n",
      "iteration 0 / 1500: loss 677.631911\n",
      "iteration 100 / 1500: loss 668.567095\n",
      "iteration 200 / 1500: loss 659.125101\n",
      "iteration 300 / 1500: loss 650.488827\n",
      "iteration 400 / 1500: loss 641.404211\n",
      "iteration 500 / 1500: loss 632.197243\n",
      "iteration 600 / 1500: loss 623.480257\n",
      "iteration 700 / 1500: loss 614.856379\n",
      "iteration 800 / 1500: loss 607.048757\n",
      "iteration 900 / 1500: loss 598.614525\n",
      "iteration 1000 / 1500: loss 590.522993\n",
      "iteration 1100 / 1500: loss 582.368077\n",
      "iteration 1200 / 1500: loss 574.535509\n",
      "iteration 1300 / 1500: loss 566.894879\n",
      "iteration 1400 / 1500: loss 558.820692\n",
      "3.16227766017e-09 46415.8883361\n",
      "iteration 0 / 1500: loss 1416.693465\n",
      "iteration 100 / 1500: loss 1375.391097\n",
      "iteration 200 / 1500: loss 1336.017563\n",
      "iteration 300 / 1500: loss 1296.328241\n",
      "iteration 400 / 1500: loss 1258.805955\n",
      "iteration 500 / 1500: loss 1222.391102\n",
      "iteration 600 / 1500: loss 1186.933942\n",
      "iteration 700 / 1500: loss 1152.532276\n",
      "iteration 800 / 1500: loss 1119.221784\n",
      "iteration 900 / 1500: loss 1086.957116\n",
      "iteration 1000 / 1500: loss 1054.946162\n",
      "iteration 1100 / 1500: loss 1024.533152\n",
      "iteration 1200 / 1500: loss 994.980597\n",
      "iteration 1300 / 1500: loss 965.800192\n",
      "iteration 1400 / 1500: loss 938.050043\n",
      "3.16227766017e-09 100000.0\n",
      "iteration 0 / 1500: loss 3009.483739\n",
      "iteration 100 / 1500: loss 2824.877203\n",
      "iteration 200 / 1500: loss 2650.957726\n",
      "iteration 300 / 1500: loss 2488.120666\n",
      "iteration 400 / 1500: loss 2335.769785\n",
      "iteration 500 / 1500: loss 2192.575087\n",
      "iteration 600 / 1500: loss 2058.186316\n",
      "iteration 700 / 1500: loss 1931.353923\n",
      "iteration 800 / 1500: loss 1813.409756\n",
      "iteration 900 / 1500: loss 1702.038147\n",
      "iteration 1000 / 1500: loss 1597.776330\n",
      "iteration 1100 / 1500: loss 1499.731840\n",
      "iteration 1200 / 1500: loss 1407.588620\n",
      "iteration 1300 / 1500: loss 1321.617608\n",
      "iteration 1400 / 1500: loss 1240.359555\n",
      "3.16227766017e-09 215443.469003\n",
      "iteration 0 / 1500: loss 6736.823840\n",
      "iteration 100 / 1500: loss 5877.933920\n",
      "iteration 200 / 1500: loss 5128.730023\n",
      "iteration 300 / 1500: loss 4475.001081\n",
      "iteration 400 / 1500: loss 3904.658316\n",
      "iteration 500 / 1500: loss 3406.658755\n",
      "iteration 600 / 1500: loss 2972.459875\n",
      "iteration 700 / 1500: loss 2593.312740\n",
      "iteration 800 / 1500: loss 2263.059835\n",
      "iteration 900 / 1500: loss 1974.734397\n",
      "iteration 1000 / 1500: loss 1723.033183\n",
      "iteration 1100 / 1500: loss 1503.715169\n",
      "iteration 1200 / 1500: loss 1312.174477\n",
      "iteration 1300 / 1500: loss 1145.071369\n",
      "iteration 1400 / 1500: loss 999.300389\n",
      "3.16227766017e-09 464158.883361\n",
      "iteration 0 / 1500: loss 14208.254121\n",
      "iteration 100 / 1500: loss 10591.280820\n",
      "iteration 200 / 1500: loss 7894.927257\n",
      "iteration 300 / 1500: loss 5885.408760\n",
      "iteration 400 / 1500: loss 4387.051254\n",
      "iteration 500 / 1500: loss 3270.639920\n",
      "iteration 600 / 1500: loss 2438.434103\n",
      "iteration 700 / 1500: loss 1818.036533\n",
      "iteration 800 / 1500: loss 1355.610000\n",
      "iteration 900 / 1500: loss 1011.000303\n",
      "iteration 1000 / 1500: loss 754.160670\n",
      "iteration 1100 / 1500: loss 562.631566\n",
      "iteration 1200 / 1500: loss 419.933183\n",
      "iteration 1300 / 1500: loss 313.584829\n",
      "iteration 1400 / 1500: loss 234.290695\n",
      "3.16227766017e-09 1000000.0\n",
      "iteration 0 / 1500: loss 31082.442829\n",
      "iteration 100 / 1500: loss 16496.725621\n",
      "iteration 200 / 1500: loss 8755.694005\n",
      "iteration 300 / 1500: loss 4647.525764\n",
      "iteration 400 / 1500: loss 2467.443437\n",
      "iteration 500 / 1500: loss 1310.378140\n",
      "iteration 600 / 1500: loss 696.435928\n",
      "iteration 700 / 1500: loss 370.714498\n",
      "iteration 800 / 1500: loss 197.788894\n",
      "iteration 900 / 1500: loss 106.047604\n",
      "iteration 1000 / 1500: loss 57.361281\n",
      "iteration 1100 / 1500: loss 31.500233\n",
      "iteration 1200 / 1500: loss 17.803989\n",
      "iteration 1300 / 1500: loss 10.524511\n",
      "iteration 1400 / 1500: loss 6.662315\n",
      "1e-08 100.0\n",
      "iteration 0 / 1500: loss 8.432575\n",
      "iteration 100 / 1500: loss 7.529691\n",
      "iteration 200 / 1500: loss 7.534058\n",
      "iteration 300 / 1500: loss 7.285852\n",
      "iteration 400 / 1500: loss 7.264878\n",
      "iteration 500 / 1500: loss 7.105027\n",
      "iteration 600 / 1500: loss 6.913330\n",
      "iteration 700 / 1500: loss 7.248204\n",
      "iteration 800 / 1500: loss 6.997251\n",
      "iteration 900 / 1500: loss 7.004006\n",
      "iteration 1000 / 1500: loss 7.123698\n",
      "iteration 1100 / 1500: loss 6.828480\n",
      "iteration 1200 / 1500: loss 6.972269\n",
      "iteration 1300 / 1500: loss 6.616482\n",
      "iteration 1400 / 1500: loss 6.565538\n",
      "1e-08 215.443469003\n",
      "iteration 0 / 1500: loss 11.775659\n",
      "iteration 100 / 1500: loss 11.375715\n",
      "iteration 200 / 1500: loss 11.205063\n",
      "iteration 300 / 1500: loss 11.199909\n",
      "iteration 400 / 1500: loss 11.241450\n",
      "iteration 500 / 1500: loss 10.701283\n",
      "iteration 600 / 1500: loss 11.065382\n",
      "iteration 700 / 1500: loss 10.956953\n",
      "iteration 800 / 1500: loss 10.303680\n",
      "iteration 900 / 1500: loss 10.373562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 / 1500: loss 10.538462\n",
      "iteration 1100 / 1500: loss 10.844959\n",
      "iteration 1200 / 1500: loss 10.564365\n",
      "iteration 1300 / 1500: loss 10.348932\n",
      "iteration 1400 / 1500: loss 10.529911\n",
      "1e-08 464.158883361\n",
      "iteration 0 / 1500: loss 20.062582\n",
      "iteration 100 / 1500: loss 19.133818\n",
      "iteration 200 / 1500: loss 19.294784\n",
      "iteration 300 / 1500: loss 18.542324\n",
      "iteration 400 / 1500: loss 19.050341\n",
      "iteration 500 / 1500: loss 19.014070\n",
      "iteration 600 / 1500: loss 18.645329\n",
      "iteration 700 / 1500: loss 18.272247\n",
      "iteration 800 / 1500: loss 18.014738\n",
      "iteration 900 / 1500: loss 18.213203\n",
      "iteration 1000 / 1500: loss 18.030364\n",
      "iteration 1100 / 1500: loss 17.830419\n",
      "iteration 1200 / 1500: loss 17.866603\n",
      "iteration 1300 / 1500: loss 18.012766\n",
      "iteration 1400 / 1500: loss 17.901287\n",
      "1e-08 1000.0\n",
      "iteration 0 / 1500: loss 37.303765\n",
      "iteration 100 / 1500: loss 36.826848\n",
      "iteration 200 / 1500: loss 36.133162\n",
      "iteration 300 / 1500: loss 35.896241\n",
      "iteration 400 / 1500: loss 35.221835\n",
      "iteration 500 / 1500: loss 34.983773\n",
      "iteration 600 / 1500: loss 34.829557\n",
      "iteration 700 / 1500: loss 34.989752\n",
      "iteration 800 / 1500: loss 34.493645\n",
      "iteration 900 / 1500: loss 34.593069\n",
      "iteration 1000 / 1500: loss 34.786556\n",
      "iteration 1100 / 1500: loss 33.931423\n",
      "iteration 1200 / 1500: loss 33.776194\n",
      "iteration 1300 / 1500: loss 33.784080\n",
      "iteration 1400 / 1500: loss 33.800150\n",
      "1e-08 2154.43469003\n",
      "iteration 0 / 1500: loss 73.548853\n",
      "iteration 100 / 1500: loss 73.068031\n",
      "iteration 200 / 1500: loss 72.211436\n",
      "iteration 300 / 1500: loss 71.220983\n",
      "iteration 400 / 1500: loss 70.997169\n",
      "iteration 500 / 1500: loss 70.556078\n",
      "iteration 600 / 1500: loss 70.107594\n",
      "iteration 700 / 1500: loss 69.476941\n",
      "iteration 800 / 1500: loss 69.181385\n",
      "iteration 900 / 1500: loss 68.772248\n",
      "iteration 1000 / 1500: loss 68.477535\n",
      "iteration 1100 / 1500: loss 67.946518\n",
      "iteration 1200 / 1500: loss 67.703375\n",
      "iteration 1300 / 1500: loss 67.315535\n",
      "iteration 1400 / 1500: loss 67.099728\n",
      "1e-08 4641.58883361\n",
      "iteration 0 / 1500: loss 146.922240\n",
      "iteration 100 / 1500: loss 145.338275\n",
      "iteration 200 / 1500: loss 143.426526\n",
      "iteration 300 / 1500: loss 142.212419\n",
      "iteration 400 / 1500: loss 140.831352\n",
      "iteration 500 / 1500: loss 139.215415\n",
      "iteration 600 / 1500: loss 137.727934\n",
      "iteration 700 / 1500: loss 136.537862\n",
      "iteration 800 / 1500: loss 135.209208\n",
      "iteration 900 / 1500: loss 133.881027\n",
      "iteration 1000 / 1500: loss 132.728966\n",
      "iteration 1100 / 1500: loss 131.234904\n",
      "iteration 1200 / 1500: loss 130.282695\n",
      "iteration 1300 / 1500: loss 128.782711\n",
      "iteration 1400 / 1500: loss 127.983262\n",
      "1e-08 10000.0\n",
      "iteration 0 / 1500: loss 315.875025\n",
      "iteration 100 / 1500: loss 309.434819\n",
      "iteration 200 / 1500: loss 302.765438\n",
      "iteration 300 / 1500: loss 296.403840\n",
      "iteration 400 / 1500: loss 290.776198\n",
      "iteration 500 / 1500: loss 284.278837\n",
      "iteration 600 / 1500: loss 278.642724\n",
      "iteration 700 / 1500: loss 273.016786\n",
      "iteration 800 / 1500: loss 267.449951\n",
      "iteration 900 / 1500: loss 261.867480\n",
      "iteration 1000 / 1500: loss 257.181822\n",
      "iteration 1100 / 1500: loss 251.747548\n",
      "iteration 1200 / 1500: loss 246.696043\n",
      "iteration 1300 / 1500: loss 241.593764\n",
      "iteration 1400 / 1500: loss 236.835956\n",
      "1e-08 21544.3469003\n",
      "iteration 0 / 1500: loss 660.174028\n",
      "iteration 100 / 1500: loss 631.959366\n",
      "iteration 200 / 1500: loss 605.141897\n",
      "iteration 300 / 1500: loss 579.816672\n",
      "iteration 400 / 1500: loss 554.639904\n",
      "iteration 500 / 1500: loss 531.016468\n",
      "iteration 600 / 1500: loss 508.322633\n",
      "iteration 700 / 1500: loss 487.069638\n",
      "iteration 800 / 1500: loss 466.339940\n",
      "iteration 900 / 1500: loss 446.353649\n",
      "iteration 1000 / 1500: loss 427.568043\n",
      "iteration 1100 / 1500: loss 409.741044\n",
      "iteration 1200 / 1500: loss 392.219919\n",
      "iteration 1300 / 1500: loss 375.755172\n",
      "iteration 1400 / 1500: loss 359.625773\n",
      "1e-08 46415.8883361\n",
      "iteration 0 / 1500: loss 1441.308928\n",
      "iteration 100 / 1500: loss 1312.373691\n",
      "iteration 200 / 1500: loss 1196.050580\n",
      "iteration 300 / 1500: loss 1089.826436\n",
      "iteration 400 / 1500: loss 992.647721\n",
      "iteration 500 / 1500: loss 904.501655\n",
      "iteration 600 / 1500: loss 824.194442\n",
      "iteration 700 / 1500: loss 750.764641\n",
      "iteration 800 / 1500: loss 684.394859\n",
      "iteration 900 / 1500: loss 623.665987\n",
      "iteration 1000 / 1500: loss 568.357614\n",
      "iteration 1100 / 1500: loss 518.006948\n",
      "iteration 1200 / 1500: loss 472.403557\n",
      "iteration 1300 / 1500: loss 430.425608\n",
      "iteration 1400 / 1500: loss 392.067664\n",
      "1e-08 100000.0\n",
      "iteration 0 / 1500: loss 3106.468322\n",
      "iteration 100 / 1500: loss 2542.848598\n",
      "iteration 200 / 1500: loss 2081.156780\n",
      "iteration 300 / 1500: loss 1703.508610\n",
      "iteration 400 / 1500: loss 1394.165857\n",
      "iteration 500 / 1500: loss 1141.330979\n",
      "iteration 600 / 1500: loss 934.361766\n",
      "iteration 700 / 1500: loss 764.970876\n",
      "iteration 800 / 1500: loss 626.442742\n",
      "iteration 900 / 1500: loss 513.068851\n",
      "iteration 1000 / 1500: loss 420.212435\n",
      "iteration 1100 / 1500: loss 344.265439\n",
      "iteration 1200 / 1500: loss 282.102819\n",
      "iteration 1300 / 1500: loss 231.376312\n",
      "iteration 1400 / 1500: loss 189.673714\n",
      "1e-08 215443.469003\n",
      "iteration 0 / 1500: loss 6591.430027\n",
      "iteration 100 / 1500: loss 4281.432827\n",
      "iteration 200 / 1500: loss 2781.184103\n",
      "iteration 300 / 1500: loss 1806.598266\n",
      "iteration 400 / 1500: loss 1174.053041\n",
      "iteration 500 / 1500: loss 763.066577\n",
      "iteration 600 / 1500: loss 496.427374\n",
      "iteration 700 / 1500: loss 323.084184\n",
      "iteration 800 / 1500: loss 210.549032\n",
      "iteration 900 / 1500: loss 137.620139\n",
      "iteration 1000 / 1500: loss 90.141727\n",
      "iteration 1100 / 1500: loss 59.316295\n",
      "iteration 1200 / 1500: loss 39.306737\n",
      "iteration 1300 / 1500: loss 26.312170\n",
      "iteration 1400 / 1500: loss 17.910034\n",
      "1e-08 464158.883361\n",
      "iteration 0 / 1500: loss 14157.726631\n",
      "iteration 100 / 1500: loss 5582.688998\n",
      "iteration 200 / 1500: loss 2201.929276\n",
      "iteration 300 / 1500: loss 869.204854\n",
      "iteration 400 / 1500: loss 343.942200\n",
      "iteration 500 / 1500: loss 136.965855\n",
      "iteration 600 / 1500: loss 55.403809\n",
      "iteration 700 / 1500: loss 23.198344\n",
      "iteration 800 / 1500: loss 10.534083\n",
      "iteration 900 / 1500: loss 5.507596\n",
      "iteration 1000 / 1500: loss 3.566623\n",
      "iteration 1100 / 1500: loss 2.806535\n",
      "iteration 1200 / 1500: loss 2.475884\n",
      "iteration 1300 / 1500: loss 2.339213\n",
      "iteration 1400 / 1500: loss 2.319306\n",
      "1e-08 1000000.0\n",
      "iteration 0 / 1500: loss 30794.483306\n",
      "iteration 100 / 1500: loss 4126.158042\n",
      "iteration 200 / 1500: loss 554.502194\n",
      "iteration 300 / 1500: loss 76.254276\n",
      "iteration 400 / 1500: loss 12.200753\n",
      "iteration 500 / 1500: loss 3.615752\n",
      "iteration 600 / 1500: loss 2.471827\n",
      "iteration 700 / 1500: loss 2.322386\n",
      "iteration 800 / 1500: loss 2.313082\n",
      "iteration 900 / 1500: loss 2.310396\n",
      "iteration 1000 / 1500: loss 2.311953\n",
      "iteration 1100 / 1500: loss 2.290157\n",
      "iteration 1200 / 1500: loss 2.292092\n",
      "iteration 1300 / 1500: loss 2.307971\n",
      "iteration 1400 / 1500: loss 2.302598\n",
      "3.16227766017e-08 100.0\n",
      "iteration 0 / 1500: loss 9.231105\n",
      "iteration 100 / 1500: loss 8.454457\n",
      "iteration 200 / 1500: loss 7.595705\n",
      "iteration 300 / 1500: loss 7.435481\n",
      "iteration 400 / 1500: loss 6.994702\n",
      "iteration 500 / 1500: loss 6.879642\n",
      "iteration 600 / 1500: loss 6.704064\n",
      "iteration 700 / 1500: loss 6.402471\n",
      "iteration 800 / 1500: loss 6.468940\n",
      "iteration 900 / 1500: loss 6.437525\n",
      "iteration 1000 / 1500: loss 6.310957\n",
      "iteration 1100 / 1500: loss 6.086586\n",
      "iteration 1200 / 1500: loss 6.234421\n",
      "iteration 1300 / 1500: loss 6.422888\n",
      "iteration 1400 / 1500: loss 6.080985\n",
      "3.16227766017e-08 215.443469003\n",
      "iteration 0 / 1500: loss 12.023370\n",
      "iteration 100 / 1500: loss 11.253076\n",
      "iteration 200 / 1500: loss 10.816370\n",
      "iteration 300 / 1500: loss 10.748964\n",
      "iteration 400 / 1500: loss 10.403742\n",
      "iteration 500 / 1500: loss 10.434393\n",
      "iteration 600 / 1500: loss 10.256351\n",
      "iteration 700 / 1500: loss 10.339367\n",
      "iteration 800 / 1500: loss 9.948510\n",
      "iteration 900 / 1500: loss 10.080924\n",
      "iteration 1000 / 1500: loss 9.850642\n",
      "iteration 1100 / 1500: loss 9.863272\n",
      "iteration 1200 / 1500: loss 9.809495\n",
      "iteration 1300 / 1500: loss 9.608818\n",
      "iteration 1400 / 1500: loss 9.559804\n",
      "3.16227766017e-08 464.158883361\n",
      "iteration 0 / 1500: loss 19.796212\n",
      "iteration 100 / 1500: loss 18.431166\n",
      "iteration 200 / 1500: loss 17.887606\n",
      "iteration 300 / 1500: loss 17.795267\n",
      "iteration 400 / 1500: loss 18.076048\n",
      "iteration 500 / 1500: loss 17.358473\n",
      "iteration 600 / 1500: loss 17.268987\n",
      "iteration 700 / 1500: loss 17.240496\n",
      "iteration 800 / 1500: loss 17.001789\n",
      "iteration 900 / 1500: loss 17.107060\n",
      "iteration 1000 / 1500: loss 16.801701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1100 / 1500: loss 16.853987\n",
      "iteration 1200 / 1500: loss 16.737996\n",
      "iteration 1300 / 1500: loss 16.651543\n",
      "iteration 1400 / 1500: loss 16.392511\n",
      "3.16227766017e-08 1000.0\n",
      "iteration 0 / 1500: loss 35.771502\n",
      "iteration 100 / 1500: loss 35.498216\n",
      "iteration 200 / 1500: loss 34.874668\n",
      "iteration 300 / 1500: loss 34.284102\n",
      "iteration 400 / 1500: loss 33.793969\n",
      "iteration 500 / 1500: loss 33.360047\n",
      "iteration 600 / 1500: loss 33.473709\n",
      "iteration 700 / 1500: loss 32.661431\n",
      "iteration 800 / 1500: loss 32.624406\n",
      "iteration 900 / 1500: loss 32.146394\n",
      "iteration 1000 / 1500: loss 32.043389\n",
      "iteration 1100 / 1500: loss 31.895876\n",
      "iteration 1200 / 1500: loss 31.513717\n",
      "iteration 1300 / 1500: loss 31.294821\n",
      "iteration 1400 / 1500: loss 31.273680\n",
      "3.16227766017e-08 2154.43469003\n",
      "iteration 0 / 1500: loss 72.128488\n",
      "iteration 100 / 1500: loss 69.910871\n",
      "iteration 200 / 1500: loss 69.163564\n",
      "iteration 300 / 1500: loss 67.819726\n",
      "iteration 400 / 1500: loss 66.418106\n",
      "iteration 500 / 1500: loss 65.584814\n",
      "iteration 600 / 1500: loss 64.272115\n",
      "iteration 700 / 1500: loss 63.656018\n",
      "iteration 800 / 1500: loss 62.636380\n",
      "iteration 900 / 1500: loss 61.988283\n",
      "iteration 1000 / 1500: loss 60.741926\n",
      "iteration 1100 / 1500: loss 59.939044\n",
      "iteration 1200 / 1500: loss 59.273455\n",
      "iteration 1300 / 1500: loss 58.478989\n",
      "iteration 1400 / 1500: loss 57.526303\n",
      "3.16227766017e-08 4641.58883361\n",
      "iteration 0 / 1500: loss 149.107727\n",
      "iteration 100 / 1500: loss 143.971481\n",
      "iteration 200 / 1500: loss 139.320542\n",
      "iteration 300 / 1500: loss 134.688302\n",
      "iteration 400 / 1500: loss 130.688045\n",
      "iteration 500 / 1500: loss 126.610221\n",
      "iteration 600 / 1500: loss 122.757556\n",
      "iteration 700 / 1500: loss 119.233830\n",
      "iteration 800 / 1500: loss 116.108265\n",
      "iteration 900 / 1500: loss 112.662973\n",
      "iteration 1000 / 1500: loss 109.102031\n",
      "iteration 1100 / 1500: loss 105.940479\n",
      "iteration 1200 / 1500: loss 102.958825\n",
      "iteration 1300 / 1500: loss 99.724370\n",
      "iteration 1400 / 1500: loss 96.994198\n",
      "3.16227766017e-08 10000.0\n",
      "iteration 0 / 1500: loss 317.357354\n",
      "iteration 100 / 1500: loss 297.104118\n",
      "iteration 200 / 1500: loss 278.917933\n",
      "iteration 300 / 1500: loss 261.166033\n",
      "iteration 400 / 1500: loss 244.987049\n",
      "iteration 500 / 1500: loss 229.872181\n",
      "iteration 600 / 1500: loss 215.559875\n",
      "iteration 700 / 1500: loss 202.292064\n",
      "iteration 800 / 1500: loss 190.041795\n",
      "iteration 900 / 1500: loss 178.447519\n",
      "iteration 1000 / 1500: loss 167.063127\n",
      "iteration 1100 / 1500: loss 156.953053\n",
      "iteration 1200 / 1500: loss 147.557645\n",
      "iteration 1300 / 1500: loss 138.466223\n",
      "iteration 1400 / 1500: loss 130.092660\n",
      "3.16227766017e-08 21544.3469003\n",
      "iteration 0 / 1500: loss 668.485660\n",
      "iteration 100 / 1500: loss 582.961959\n",
      "iteration 200 / 1500: loss 507.922642\n",
      "iteration 300 / 1500: loss 443.195172\n",
      "iteration 400 / 1500: loss 386.579609\n",
      "iteration 500 / 1500: loss 337.181217\n",
      "iteration 600 / 1500: loss 294.784346\n",
      "iteration 700 / 1500: loss 256.945052\n",
      "iteration 800 / 1500: loss 224.210396\n",
      "iteration 900 / 1500: loss 195.802788\n",
      "iteration 1000 / 1500: loss 171.154740\n",
      "iteration 1100 / 1500: loss 149.585875\n",
      "iteration 1200 / 1500: loss 130.502542\n",
      "iteration 1300 / 1500: loss 114.200481\n",
      "iteration 1400 / 1500: loss 99.748770\n",
      "3.16227766017e-08 46415.8883361\n",
      "iteration 0 / 1500: loss 1425.112593\n",
      "iteration 100 / 1500: loss 1061.942047\n",
      "iteration 200 / 1500: loss 791.111446\n",
      "iteration 300 / 1500: loss 589.720643\n",
      "iteration 400 / 1500: loss 439.690496\n",
      "iteration 500 / 1500: loss 328.118999\n",
      "iteration 600 / 1500: loss 245.022473\n",
      "iteration 700 / 1500: loss 182.975473\n",
      "iteration 800 / 1500: loss 136.878285\n",
      "iteration 900 / 1500: loss 102.433955\n",
      "iteration 1000 / 1500: loss 76.930752\n",
      "iteration 1100 / 1500: loss 57.803855\n",
      "iteration 1200 / 1500: loss 43.607502\n",
      "iteration 1300 / 1500: loss 33.005173\n",
      "iteration 1400 / 1500: loss 25.131634\n",
      "3.16227766017e-08 100000.0\n",
      "iteration 0 / 1500: loss 3100.374085\n",
      "iteration 100 / 1500: loss 1644.163358\n",
      "iteration 200 / 1500: loss 872.594259\n",
      "iteration 300 / 1500: loss 463.749028\n",
      "iteration 400 / 1500: loss 246.954409\n",
      "iteration 500 / 1500: loss 131.962453\n",
      "iteration 600 / 1500: loss 71.027292\n",
      "iteration 700 / 1500: loss 38.664735\n",
      "iteration 800 / 1500: loss 21.566493\n",
      "iteration 900 / 1500: loss 12.469347\n",
      "iteration 1000 / 1500: loss 7.667485\n",
      "iteration 1100 / 1500: loss 5.068367\n",
      "iteration 1200 / 1500: loss 3.753482\n",
      "iteration 1300 / 1500: loss 2.996232\n",
      "iteration 1400 / 1500: loss 2.692639\n",
      "3.16227766017e-08 215443.469003\n",
      "iteration 0 / 1500: loss 6544.626461\n",
      "iteration 100 / 1500: loss 1668.094462\n",
      "iteration 200 / 1500: loss 426.136971\n",
      "iteration 300 / 1500: loss 110.195532\n",
      "iteration 400 / 1500: loss 29.739118\n",
      "iteration 500 / 1500: loss 9.242397\n",
      "iteration 600 / 1500: loss 4.020867\n",
      "iteration 700 / 1500: loss 2.715547\n",
      "iteration 800 / 1500: loss 2.353736\n",
      "iteration 900 / 1500: loss 2.270458\n",
      "iteration 1000 / 1500: loss 2.262417\n",
      "iteration 1100 / 1500: loss 2.243939\n",
      "iteration 1200 / 1500: loss 2.264563\n",
      "iteration 1300 / 1500: loss 2.219699\n",
      "iteration 1400 / 1500: loss 2.275274\n",
      "3.16227766017e-08 464158.883361\n",
      "iteration 0 / 1500: loss 14208.600517\n",
      "iteration 100 / 1500: loss 739.524826\n",
      "iteration 200 / 1500: loss 40.564694\n",
      "iteration 300 / 1500: loss 4.264719\n",
      "iteration 400 / 1500: loss 2.393601\n",
      "iteration 500 / 1500: loss 2.265921\n",
      "iteration 600 / 1500: loss 2.267629\n",
      "iteration 700 / 1500: loss 2.278660\n",
      "iteration 800 / 1500: loss 2.285015\n",
      "iteration 900 / 1500: loss 2.270839\n",
      "iteration 1000 / 1500: loss 2.275249\n",
      "iteration 1100 / 1500: loss 2.274539\n",
      "iteration 1200 / 1500: loss 2.270406\n",
      "iteration 1300 / 1500: loss 2.269392\n",
      "iteration 1400 / 1500: loss 2.292489\n",
      "3.16227766017e-08 1000000.0\n",
      "iteration 0 / 1500: loss 30501.349704\n",
      "iteration 100 / 1500: loss 51.562207\n",
      "iteration 200 / 1500: loss 2.372788\n",
      "iteration 300 / 1500: loss 2.296304\n",
      "iteration 400 / 1500: loss 2.297454\n",
      "iteration 500 / 1500: loss 2.305391\n",
      "iteration 600 / 1500: loss 2.288651\n",
      "iteration 700 / 1500: loss 2.298512\n",
      "iteration 800 / 1500: loss 2.286049\n",
      "iteration 900 / 1500: loss 2.289153\n",
      "iteration 1000 / 1500: loss 2.297880\n",
      "iteration 1100 / 1500: loss 2.304919\n",
      "iteration 1200 / 1500: loss 2.290988\n",
      "iteration 1300 / 1500: loss 2.307414\n",
      "iteration 1400 / 1500: loss 2.307341\n",
      "1e-07 100.0\n",
      "iteration 0 / 1500: loss 9.406947\n",
      "iteration 100 / 1500: loss 7.481231\n",
      "iteration 200 / 1500: loss 7.225427\n",
      "iteration 300 / 1500: loss 6.391087\n",
      "iteration 400 / 1500: loss 6.439985\n",
      "iteration 500 / 1500: loss 6.036516\n",
      "iteration 600 / 1500: loss 5.980223\n",
      "iteration 700 / 1500: loss 5.806863\n",
      "iteration 800 / 1500: loss 5.892050\n",
      "iteration 900 / 1500: loss 6.068881\n",
      "iteration 1000 / 1500: loss 5.922182\n",
      "iteration 1100 / 1500: loss 5.599563\n",
      "iteration 1200 / 1500: loss 5.476013\n",
      "iteration 1300 / 1500: loss 5.598253\n",
      "iteration 1400 / 1500: loss 5.731087\n",
      "1e-07 215.443469003\n",
      "iteration 0 / 1500: loss 13.427708\n",
      "iteration 100 / 1500: loss 10.971191\n",
      "iteration 200 / 1500: loss 10.140403\n",
      "iteration 300 / 1500: loss 9.719271\n",
      "iteration 400 / 1500: loss 9.750666\n",
      "iteration 500 / 1500: loss 9.590691\n",
      "iteration 600 / 1500: loss 9.375123\n",
      "iteration 700 / 1500: loss 9.275732\n",
      "iteration 800 / 1500: loss 9.113516\n",
      "iteration 900 / 1500: loss 9.056556\n",
      "iteration 1000 / 1500: loss 9.223154\n",
      "iteration 1100 / 1500: loss 9.151073\n",
      "iteration 1200 / 1500: loss 8.683507\n",
      "iteration 1300 / 1500: loss 8.841744\n",
      "iteration 1400 / 1500: loss 8.821840\n",
      "1e-07 464.158883361\n",
      "iteration 0 / 1500: loss 20.608418\n",
      "iteration 100 / 1500: loss 18.542311\n",
      "iteration 200 / 1500: loss 17.538337\n",
      "iteration 300 / 1500: loss 17.184794\n",
      "iteration 400 / 1500: loss 16.906298\n",
      "iteration 500 / 1500: loss 16.458581\n",
      "iteration 600 / 1500: loss 16.236941\n",
      "iteration 700 / 1500: loss 16.060461\n",
      "iteration 800 / 1500: loss 15.949051\n",
      "iteration 900 / 1500: loss 16.217490\n",
      "iteration 1000 / 1500: loss 15.697923\n",
      "iteration 1100 / 1500: loss 15.375044\n",
      "iteration 1200 / 1500: loss 15.405404\n",
      "iteration 1300 / 1500: loss 14.877909\n",
      "iteration 1400 / 1500: loss 14.886865\n",
      "1e-07 1000.0\n",
      "iteration 0 / 1500: loss 38.491671\n",
      "iteration 100 / 1500: loss 34.840687\n",
      "iteration 200 / 1500: loss 33.576368\n",
      "iteration 300 / 1500: loss 32.907372\n",
      "iteration 400 / 1500: loss 31.442851\n",
      "iteration 500 / 1500: loss 30.829426\n",
      "iteration 600 / 1500: loss 30.119840\n",
      "iteration 700 / 1500: loss 29.512745\n",
      "iteration 800 / 1500: loss 28.955279\n",
      "iteration 900 / 1500: loss 28.136476\n",
      "iteration 1000 / 1500: loss 27.906401\n",
      "iteration 1100 / 1500: loss 27.059790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200 / 1500: loss 26.409612\n",
      "iteration 1300 / 1500: loss 26.098835\n",
      "iteration 1400 / 1500: loss 25.595291\n",
      "1e-07 2154.43469003\n",
      "iteration 0 / 1500: loss 72.296226\n",
      "iteration 100 / 1500: loss 67.120176\n",
      "iteration 200 / 1500: loss 63.994211\n",
      "iteration 300 / 1500: loss 60.993381\n",
      "iteration 400 / 1500: loss 58.316976\n",
      "iteration 500 / 1500: loss 55.790289\n",
      "iteration 600 / 1500: loss 53.337257\n",
      "iteration 700 / 1500: loss 50.808417\n",
      "iteration 800 / 1500: loss 48.735757\n",
      "iteration 900 / 1500: loss 46.738125\n",
      "iteration 1000 / 1500: loss 44.747285\n",
      "iteration 1100 / 1500: loss 43.043491\n",
      "iteration 1200 / 1500: loss 41.155687\n",
      "iteration 1300 / 1500: loss 39.617585\n",
      "iteration 1400 / 1500: loss 37.885966\n",
      "1e-07 4641.58883361\n",
      "iteration 0 / 1500: loss 148.946630\n",
      "iteration 100 / 1500: loss 134.383034\n",
      "iteration 200 / 1500: loss 122.220274\n",
      "iteration 300 / 1500: loss 110.884876\n",
      "iteration 400 / 1500: loss 100.900992\n",
      "iteration 500 / 1500: loss 91.957281\n",
      "iteration 600 / 1500: loss 83.761036\n",
      "iteration 700 / 1500: loss 76.372728\n",
      "iteration 800 / 1500: loss 69.723046\n",
      "iteration 900 / 1500: loss 63.598925\n",
      "iteration 1000 / 1500: loss 58.185906\n",
      "iteration 1100 / 1500: loss 52.906513\n",
      "iteration 1200 / 1500: loss 48.259788\n",
      "iteration 1300 / 1500: loss 44.217009\n",
      "iteration 1400 / 1500: loss 40.487740\n",
      "1e-07 10000.0\n",
      "iteration 0 / 1500: loss 313.741672\n",
      "iteration 100 / 1500: loss 256.010135\n",
      "iteration 200 / 1500: loss 209.177261\n",
      "iteration 300 / 1500: loss 171.179351\n",
      "iteration 400 / 1500: loss 140.407217\n",
      "iteration 500 / 1500: loss 115.071114\n",
      "iteration 600 / 1500: loss 94.143585\n",
      "iteration 700 / 1500: loss 77.457309\n",
      "iteration 800 / 1500: loss 63.611694\n",
      "iteration 900 / 1500: loss 52.508045\n",
      "iteration 1000 / 1500: loss 43.362663\n",
      "iteration 1100 / 1500: loss 35.651170\n",
      "iteration 1200 / 1500: loss 29.547458\n",
      "iteration 1300 / 1500: loss 24.634489\n",
      "iteration 1400 / 1500: loss 20.433717\n",
      "1e-07 21544.3469003\n",
      "iteration 0 / 1500: loss 664.842862\n",
      "iteration 100 / 1500: loss 430.375655\n",
      "iteration 200 / 1500: loss 279.745927\n",
      "iteration 300 / 1500: loss 181.989875\n",
      "iteration 400 / 1500: loss 118.670477\n",
      "iteration 500 / 1500: loss 77.686109\n",
      "iteration 600 / 1500: loss 51.079443\n",
      "iteration 700 / 1500: loss 33.873513\n",
      "iteration 800 / 1500: loss 22.656281\n",
      "iteration 900 / 1500: loss 15.531140\n",
      "iteration 1000 / 1500: loss 10.797860\n",
      "iteration 1100 / 1500: loss 7.822927\n",
      "iteration 1200 / 1500: loss 5.759607\n",
      "iteration 1300 / 1500: loss 4.493391\n",
      "iteration 1400 / 1500: loss 3.619787\n",
      "1e-07 46415.8883361\n",
      "iteration 0 / 1500: loss 1430.434260\n",
      "iteration 100 / 1500: loss 563.434214\n",
      "iteration 200 / 1500: loss 222.960267\n",
      "iteration 300 / 1500: loss 89.117843\n",
      "iteration 400 / 1500: loss 36.361943\n",
      "iteration 500 / 1500: loss 15.679236\n",
      "iteration 600 / 1500: loss 7.439729\n",
      "iteration 700 / 1500: loss 4.243581\n",
      "iteration 800 / 1500: loss 2.852938\n",
      "iteration 900 / 1500: loss 2.512656\n",
      "iteration 1000 / 1500: loss 2.296602\n",
      "iteration 1100 / 1500: loss 2.221408\n",
      "iteration 1200 / 1500: loss 2.195878\n",
      "iteration 1300 / 1500: loss 2.158605\n",
      "iteration 1400 / 1500: loss 2.177098\n",
      "1e-07 100000.0\n",
      "iteration 0 / 1500: loss 3131.378881\n",
      "iteration 100 / 1500: loss 419.956526\n",
      "iteration 200 / 1500: loss 57.984661\n",
      "iteration 300 / 1500: loss 9.641263\n",
      "iteration 400 / 1500: loss 3.261178\n",
      "iteration 500 / 1500: loss 2.380237\n",
      "iteration 600 / 1500: loss 2.242571\n",
      "iteration 700 / 1500: loss 2.228182\n",
      "iteration 800 / 1500: loss 2.210200\n",
      "iteration 900 / 1500: loss 2.216749\n",
      "iteration 1000 / 1500: loss 2.254599\n",
      "iteration 1100 / 1500: loss 2.259024\n",
      "iteration 1200 / 1500: loss 2.247147\n",
      "iteration 1300 / 1500: loss 2.233586\n",
      "iteration 1400 / 1500: loss 2.252136\n",
      "1e-07 215443.469003\n",
      "iteration 0 / 1500: loss 6505.460793\n",
      "iteration 100 / 1500: loss 85.362369\n",
      "iteration 200 / 1500: loss 3.329918\n",
      "iteration 300 / 1500: loss 2.276548\n",
      "iteration 400 / 1500: loss 2.263952\n",
      "iteration 500 / 1500: loss 2.260954\n",
      "iteration 600 / 1500: loss 2.253255\n",
      "iteration 700 / 1500: loss 2.270800\n",
      "iteration 800 / 1500: loss 2.241301\n",
      "iteration 900 / 1500: loss 2.274756\n",
      "iteration 1000 / 1500: loss 2.254125\n",
      "iteration 1100 / 1500: loss 2.278299\n",
      "iteration 1200 / 1500: loss 2.242776\n",
      "iteration 1300 / 1500: loss 2.242893\n",
      "iteration 1400 / 1500: loss 2.273384\n",
      "1e-07 464158.883361\n",
      "iteration 0 / 1500: loss 14293.668547\n",
      "iteration 100 / 1500: loss 3.344662\n",
      "iteration 200 / 1500: loss 2.285842\n",
      "iteration 300 / 1500: loss 2.270102\n",
      "iteration 400 / 1500: loss 2.294918\n",
      "iteration 500 / 1500: loss 2.301219\n",
      "iteration 600 / 1500: loss 2.282413\n",
      "iteration 700 / 1500: loss 2.305929\n",
      "iteration 800 / 1500: loss 2.259981\n",
      "iteration 900 / 1500: loss 2.284330\n",
      "iteration 1000 / 1500: loss 2.279514\n",
      "iteration 1100 / 1500: loss 2.289318\n",
      "iteration 1200 / 1500: loss 2.301423\n",
      "iteration 1300 / 1500: loss 2.298774\n",
      "iteration 1400 / 1500: loss 2.281725\n",
      "1e-07 1000000.0\n",
      "iteration 0 / 1500: loss 30433.998313\n",
      "iteration 100 / 1500: loss 2.289616\n",
      "iteration 200 / 1500: loss 2.301712\n",
      "iteration 300 / 1500: loss 2.298147\n",
      "iteration 400 / 1500: loss 2.289460\n",
      "iteration 500 / 1500: loss 2.294303\n",
      "iteration 600 / 1500: loss 2.306410\n",
      "iteration 700 / 1500: loss 2.297593\n",
      "iteration 800 / 1500: loss 2.321539\n",
      "iteration 900 / 1500: loss 2.302283\n",
      "iteration 1000 / 1500: loss 2.306355\n",
      "iteration 1100 / 1500: loss 2.298080\n",
      "iteration 1200 / 1500: loss 2.295621\n",
      "iteration 1300 / 1500: loss 2.301653\n",
      "iteration 1400 / 1500: loss 2.299361\n",
      "3.16227766017e-07 100.0\n",
      "iteration 0 / 1500: loss 8.500389\n",
      "iteration 100 / 1500: loss 6.464445\n",
      "iteration 200 / 1500: loss 5.998240\n",
      "iteration 300 / 1500: loss 5.916216\n",
      "iteration 400 / 1500: loss 5.662092\n",
      "iteration 500 / 1500: loss 5.668962\n",
      "iteration 600 / 1500: loss 5.310833\n",
      "iteration 700 / 1500: loss 5.465961\n",
      "iteration 800 / 1500: loss 5.248514\n",
      "iteration 900 / 1500: loss 5.116167\n",
      "iteration 1000 / 1500: loss 5.175518\n",
      "iteration 1100 / 1500: loss 5.176439\n",
      "iteration 1200 / 1500: loss 4.965747\n",
      "iteration 1300 / 1500: loss 4.972011\n",
      "iteration 1400 / 1500: loss 4.889186\n",
      "3.16227766017e-07 215.443469003\n",
      "iteration 0 / 1500: loss 11.996972\n",
      "iteration 100 / 1500: loss 9.668423\n",
      "iteration 200 / 1500: loss 9.210884\n",
      "iteration 300 / 1500: loss 9.136111\n",
      "iteration 400 / 1500: loss 9.007160\n",
      "iteration 500 / 1500: loss 8.596260\n",
      "iteration 600 / 1500: loss 8.481027\n",
      "iteration 700 / 1500: loss 8.358121\n",
      "iteration 800 / 1500: loss 8.228440\n",
      "iteration 900 / 1500: loss 8.172930\n",
      "iteration 1000 / 1500: loss 7.735900\n",
      "iteration 1100 / 1500: loss 7.635622\n",
      "iteration 1200 / 1500: loss 7.661840\n",
      "iteration 1300 / 1500: loss 7.588906\n",
      "iteration 1400 / 1500: loss 7.435091\n",
      "3.16227766017e-07 464.158883361\n",
      "iteration 0 / 1500: loss 19.825913\n",
      "iteration 100 / 1500: loss 16.758733\n",
      "iteration 200 / 1500: loss 16.251073\n",
      "iteration 300 / 1500: loss 15.459789\n",
      "iteration 400 / 1500: loss 14.858914\n",
      "iteration 500 / 1500: loss 14.460574\n",
      "iteration 600 / 1500: loss 14.023598\n",
      "iteration 700 / 1500: loss 13.813866\n",
      "iteration 800 / 1500: loss 12.910917\n",
      "iteration 900 / 1500: loss 12.795947\n",
      "iteration 1000 / 1500: loss 12.631400\n",
      "iteration 1100 / 1500: loss 12.010811\n",
      "iteration 1200 / 1500: loss 11.888479\n",
      "iteration 1300 / 1500: loss 11.436728\n",
      "iteration 1400 / 1500: loss 11.102952\n",
      "3.16227766017e-07 1000.0\n",
      "iteration 0 / 1500: loss 36.001583\n",
      "iteration 100 / 1500: loss 32.077277\n",
      "iteration 200 / 1500: loss 29.716129\n",
      "iteration 300 / 1500: loss 27.663460\n",
      "iteration 400 / 1500: loss 26.105294\n",
      "iteration 500 / 1500: loss 24.596603\n",
      "iteration 600 / 1500: loss 23.023302\n",
      "iteration 700 / 1500: loss 21.742550\n",
      "iteration 800 / 1500: loss 20.420025\n",
      "iteration 900 / 1500: loss 19.119828\n",
      "iteration 1000 / 1500: loss 18.001301\n",
      "iteration 1100 / 1500: loss 17.098294\n",
      "iteration 1200 / 1500: loss 15.912552\n",
      "iteration 1300 / 1500: loss 15.154749\n",
      "iteration 1400 / 1500: loss 14.250318\n",
      "3.16227766017e-07 2154.43469003\n",
      "iteration 0 / 1500: loss 73.117510\n",
      "iteration 100 / 1500: loss 61.226557\n",
      "iteration 200 / 1500: loss 53.061322\n",
      "iteration 300 / 1500: loss 46.194330\n",
      "iteration 400 / 1500: loss 40.434024\n",
      "iteration 500 / 1500: loss 35.428768\n",
      "iteration 600 / 1500: loss 30.936636\n",
      "iteration 700 / 1500: loss 27.216103\n",
      "iteration 800 / 1500: loss 23.917527\n",
      "iteration 900 / 1500: loss 20.912212\n",
      "iteration 1000 / 1500: loss 18.495673\n",
      "iteration 1100 / 1500: loss 16.265570\n",
      "iteration 1200 / 1500: loss 14.377124\n",
      "iteration 1300 / 1500: loss 12.982008\n",
      "iteration 1400 / 1500: loss 11.398983\n",
      "3.16227766017e-07 4641.58883361\n",
      "iteration 0 / 1500: loss 149.045679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss 108.901292\n",
      "iteration 200 / 1500: loss 81.161383\n",
      "iteration 300 / 1500: loss 60.703322\n",
      "iteration 400 / 1500: loss 45.560939\n",
      "iteration 500 / 1500: loss 34.366562\n",
      "iteration 600 / 1500: loss 25.953714\n",
      "iteration 700 / 1500: loss 19.844267\n",
      "iteration 800 / 1500: loss 15.052736\n",
      "iteration 900 / 1500: loss 11.753255\n",
      "iteration 1000 / 1500: loss 9.316016\n",
      "iteration 1100 / 1500: loss 7.496868\n",
      "iteration 1200 / 1500: loss 6.075082\n",
      "iteration 1300 / 1500: loss 4.949881\n",
      "iteration 1400 / 1500: loss 4.162346\n",
      "3.16227766017e-07 10000.0\n",
      "iteration 0 / 1500: loss 311.871946\n",
      "iteration 100 / 1500: loss 164.755121\n",
      "iteration 200 / 1500: loss 87.892481\n",
      "iteration 300 / 1500: loss 47.311497\n",
      "iteration 400 / 1500: loss 25.950180\n",
      "iteration 500 / 1500: loss 14.676286\n",
      "iteration 600 / 1500: loss 8.684497\n",
      "iteration 700 / 1500: loss 5.509213\n",
      "iteration 800 / 1500: loss 3.893188\n",
      "iteration 900 / 1500: loss 3.029408\n",
      "iteration 1000 / 1500: loss 2.542694\n",
      "iteration 1100 / 1500: loss 2.324609\n",
      "iteration 1200 / 1500: loss 2.173738\n",
      "iteration 1300 / 1500: loss 2.028454\n",
      "iteration 1400 / 1500: loss 2.173748\n",
      "3.16227766017e-07 21544.3469003\n",
      "iteration 0 / 1500: loss 661.441284\n",
      "iteration 100 / 1500: loss 168.588681\n",
      "iteration 200 / 1500: loss 44.225186\n",
      "iteration 300 / 1500: loss 12.758924\n",
      "iteration 400 / 1500: loss 4.823014\n",
      "iteration 500 / 1500: loss 2.806104\n",
      "iteration 600 / 1500: loss 2.240855\n",
      "iteration 700 / 1500: loss 2.129422\n",
      "iteration 800 / 1500: loss 2.170288\n",
      "iteration 900 / 1500: loss 2.081923\n",
      "iteration 1000 / 1500: loss 2.151608\n",
      "iteration 1100 / 1500: loss 2.117575\n",
      "iteration 1200 / 1500: loss 2.097471\n",
      "iteration 1300 / 1500: loss 2.125531\n",
      "iteration 1400 / 1500: loss 2.114033\n",
      "3.16227766017e-07 46415.8883361\n",
      "iteration 0 / 1500: loss 1447.277725\n",
      "iteration 100 / 1500: loss 76.384465\n",
      "iteration 200 / 1500: loss 5.960572\n",
      "iteration 300 / 1500: loss 2.366005\n",
      "iteration 400 / 1500: loss 2.213131\n",
      "iteration 500 / 1500: loss 2.145404\n",
      "iteration 600 / 1500: loss 2.206237\n",
      "iteration 700 / 1500: loss 2.101605\n",
      "iteration 800 / 1500: loss 2.114954\n",
      "iteration 900 / 1500: loss 2.176520\n",
      "iteration 1000 / 1500: loss 2.122122\n",
      "iteration 1100 / 1500: loss 2.107622\n",
      "iteration 1200 / 1500: loss 2.125836\n",
      "iteration 1300 / 1500: loss 2.233535\n",
      "iteration 1400 / 1500: loss 2.193488\n",
      "3.16227766017e-07 100000.0\n",
      "iteration 0 / 1500: loss 3014.273705\n",
      "iteration 100 / 1500: loss 7.062774\n",
      "iteration 200 / 1500: loss 2.263341\n",
      "iteration 300 / 1500: loss 2.232922\n",
      "iteration 400 / 1500: loss 2.228695\n",
      "iteration 500 / 1500: loss 2.213160\n",
      "iteration 600 / 1500: loss 2.174824\n",
      "iteration 700 / 1500: loss 2.234917\n",
      "iteration 800 / 1500: loss 2.230101\n",
      "iteration 900 / 1500: loss 2.210932\n",
      "iteration 1000 / 1500: loss 2.252679\n",
      "iteration 1100 / 1500: loss 2.244773\n",
      "iteration 1200 / 1500: loss 2.223794\n",
      "iteration 1300 / 1500: loss 2.212430\n",
      "iteration 1400 / 1500: loss 2.201888\n",
      "3.16227766017e-07 215443.469003\n",
      "iteration 0 / 1500: loss 6640.541601\n",
      "iteration 100 / 1500: loss 2.264291\n",
      "iteration 200 / 1500: loss 2.271032\n",
      "iteration 300 / 1500: loss 2.266717\n",
      "iteration 400 / 1500: loss 2.276625\n",
      "iteration 500 / 1500: loss 2.246463\n",
      "iteration 600 / 1500: loss 2.222421\n",
      "iteration 700 / 1500: loss 2.299335\n",
      "iteration 800 / 1500: loss 2.321351\n",
      "iteration 900 / 1500: loss 2.305278\n",
      "iteration 1000 / 1500: loss 2.237544\n",
      "iteration 1100 / 1500: loss 2.279166\n",
      "iteration 1200 / 1500: loss 2.280192\n",
      "iteration 1300 / 1500: loss 2.240762\n",
      "iteration 1400 / 1500: loss 2.249020\n",
      "3.16227766017e-07 464158.883361\n",
      "iteration 0 / 1500: loss 14247.514016\n",
      "iteration 100 / 1500: loss 2.283687\n",
      "iteration 200 / 1500: loss 2.294184\n",
      "iteration 300 / 1500: loss 2.287844\n",
      "iteration 400 / 1500: loss 2.287304\n",
      "iteration 500 / 1500: loss 2.263576\n",
      "iteration 600 / 1500: loss 2.295969\n",
      "iteration 700 / 1500: loss 2.325444\n",
      "iteration 800 / 1500: loss 2.305244\n",
      "iteration 900 / 1500: loss 2.285066\n",
      "iteration 1000 / 1500: loss 2.287302\n",
      "iteration 1100 / 1500: loss 2.292322\n",
      "iteration 1200 / 1500: loss 2.293110\n",
      "iteration 1300 / 1500: loss 2.269190\n",
      "iteration 1400 / 1500: loss 2.309597\n",
      "3.16227766017e-07 1000000.0\n",
      "iteration 0 / 1500: loss 31033.294077\n",
      "iteration 100 / 1500: loss 2.309349\n",
      "iteration 200 / 1500: loss 2.292813\n",
      "iteration 300 / 1500: loss 2.307568\n",
      "iteration 400 / 1500: loss 2.301336\n",
      "iteration 500 / 1500: loss 2.303691\n",
      "iteration 600 / 1500: loss 2.313103\n",
      "iteration 700 / 1500: loss 2.318895\n",
      "iteration 800 / 1500: loss 2.293777\n",
      "iteration 900 / 1500: loss 2.303461\n",
      "iteration 1000 / 1500: loss 2.305931\n",
      "iteration 1100 / 1500: loss 2.313635\n",
      "iteration 1200 / 1500: loss 2.311197\n",
      "iteration 1300 / 1500: loss 2.306038\n",
      "iteration 1400 / 1500: loss 2.303547\n",
      "1e-06 100.0\n",
      "iteration 0 / 1500: loss 8.292142\n",
      "iteration 100 / 1500: loss 5.701337\n",
      "iteration 200 / 1500: loss 5.377803\n",
      "iteration 300 / 1500: loss 5.090899\n",
      "iteration 400 / 1500: loss 5.032688\n",
      "iteration 500 / 1500: loss 5.001382\n",
      "iteration 600 / 1500: loss 4.789617\n",
      "iteration 700 / 1500: loss 4.582902\n",
      "iteration 800 / 1500: loss 4.457499\n",
      "iteration 900 / 1500: loss 4.411739\n",
      "iteration 1000 / 1500: loss 4.420828\n",
      "iteration 1100 / 1500: loss 4.306741\n",
      "iteration 1200 / 1500: loss 4.325512\n",
      "iteration 1300 / 1500: loss 4.231633\n",
      "iteration 1400 / 1500: loss 4.091718\n",
      "1e-06 215.443469003\n",
      "iteration 0 / 1500: loss 12.358140\n",
      "iteration 100 / 1500: loss 9.216260\n",
      "iteration 200 / 1500: loss 8.234387\n",
      "iteration 300 / 1500: loss 7.839187\n",
      "iteration 400 / 1500: loss 7.470048\n",
      "iteration 500 / 1500: loss 7.353513\n",
      "iteration 600 / 1500: loss 6.857802\n",
      "iteration 700 / 1500: loss 6.734749\n",
      "iteration 800 / 1500: loss 6.468156\n",
      "iteration 900 / 1500: loss 6.232074\n",
      "iteration 1000 / 1500: loss 5.789776\n",
      "iteration 1100 / 1500: loss 5.690354\n",
      "iteration 1200 / 1500: loss 5.516666\n",
      "iteration 1300 / 1500: loss 5.415256\n",
      "iteration 1400 / 1500: loss 5.342078\n",
      "1e-06 464.158883361\n",
      "iteration 0 / 1500: loss 19.982564\n",
      "iteration 100 / 1500: loss 15.794100\n",
      "iteration 200 / 1500: loss 13.946796\n",
      "iteration 300 / 1500: loss 12.681770\n",
      "iteration 400 / 1500: loss 11.720676\n",
      "iteration 500 / 1500: loss 10.612330\n",
      "iteration 600 / 1500: loss 9.824537\n",
      "iteration 700 / 1500: loss 9.119304\n",
      "iteration 800 / 1500: loss 8.463592\n",
      "iteration 900 / 1500: loss 7.838797\n",
      "iteration 1000 / 1500: loss 7.154619\n",
      "iteration 1100 / 1500: loss 6.727181\n",
      "iteration 1200 / 1500: loss 6.191409\n",
      "iteration 1300 / 1500: loss 5.890084\n",
      "iteration 1400 / 1500: loss 5.303008\n",
      "1e-06 1000.0\n",
      "iteration 0 / 1500: loss 36.098268\n",
      "iteration 100 / 1500: loss 27.535015\n",
      "iteration 200 / 1500: loss 22.364490\n",
      "iteration 300 / 1500: loss 18.562155\n",
      "iteration 400 / 1500: loss 15.247632\n",
      "iteration 500 / 1500: loss 12.907759\n",
      "iteration 600 / 1500: loss 10.801836\n",
      "iteration 700 / 1500: loss 9.120013\n",
      "iteration 800 / 1500: loss 7.652781\n",
      "iteration 900 / 1500: loss 6.624659\n",
      "iteration 1000 / 1500: loss 5.657505\n",
      "iteration 1100 / 1500: loss 5.020147\n",
      "iteration 1200 / 1500: loss 4.470215\n",
      "iteration 1300 / 1500: loss 3.999633\n",
      "iteration 1400 / 1500: loss 3.611639\n",
      "1e-06 2154.43469003\n",
      "iteration 0 / 1500: loss 71.430284\n",
      "iteration 100 / 1500: loss 44.782893\n",
      "iteration 200 / 1500: loss 29.314709\n",
      "iteration 300 / 1500: loss 19.571672\n",
      "iteration 400 / 1500: loss 13.362280\n",
      "iteration 500 / 1500: loss 9.227662\n",
      "iteration 600 / 1500: loss 6.755633\n",
      "iteration 700 / 1500: loss 4.955261\n",
      "iteration 800 / 1500: loss 3.857184\n",
      "iteration 900 / 1500: loss 3.046817\n",
      "iteration 1000 / 1500: loss 2.641703\n",
      "iteration 1100 / 1500: loss 2.513041\n",
      "iteration 1200 / 1500: loss 2.148256\n",
      "iteration 1300 / 1500: loss 2.046527\n",
      "iteration 1400 / 1500: loss 2.068422\n",
      "1e-06 4641.58883361\n",
      "iteration 0 / 1500: loss 148.559715\n",
      "iteration 100 / 1500: loss 58.107738\n",
      "iteration 200 / 1500: loss 23.822701\n",
      "iteration 300 / 1500: loss 10.425766\n",
      "iteration 400 / 1500: loss 5.297660\n",
      "iteration 500 / 1500: loss 3.252835\n",
      "iteration 600 / 1500: loss 2.500207\n",
      "iteration 700 / 1500: loss 2.130248\n",
      "iteration 800 / 1500: loss 2.036360\n",
      "iteration 900 / 1500: loss 1.915032\n",
      "iteration 1000 / 1500: loss 1.885024\n",
      "iteration 1100 / 1500: loss 2.081453\n",
      "iteration 1200 / 1500: loss 2.096577\n",
      "iteration 1300 / 1500: loss 2.005429\n",
      "iteration 1400 / 1500: loss 1.972571\n",
      "1e-06 10000.0\n",
      "iteration 0 / 1500: loss 315.065146\n",
      "iteration 100 / 1500: loss 42.769572\n",
      "iteration 200 / 1500: loss 7.347965\n",
      "iteration 300 / 1500: loss 2.912040\n",
      "iteration 400 / 1500: loss 2.114354\n",
      "iteration 500 / 1500: loss 2.066874\n",
      "iteration 600 / 1500: loss 2.068852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 700 / 1500: loss 2.075765\n",
      "iteration 800 / 1500: loss 2.049857\n",
      "iteration 900 / 1500: loss 2.023190\n",
      "iteration 1000 / 1500: loss 2.062578\n",
      "iteration 1100 / 1500: loss 2.000925\n",
      "iteration 1200 / 1500: loss 2.082044\n",
      "iteration 1300 / 1500: loss 2.073399\n",
      "iteration 1400 / 1500: loss 2.025020\n",
      "1e-06 21544.3469003\n",
      "iteration 0 / 1500: loss 661.333987\n",
      "iteration 100 / 1500: loss 10.365058\n",
      "iteration 200 / 1500: loss 2.250955\n",
      "iteration 300 / 1500: loss 2.141926\n",
      "iteration 400 / 1500: loss 2.140873\n",
      "iteration 500 / 1500: loss 2.125225\n",
      "iteration 600 / 1500: loss 2.140549\n",
      "iteration 700 / 1500: loss 2.125663\n",
      "iteration 800 / 1500: loss 2.149433\n",
      "iteration 900 / 1500: loss 2.046854\n",
      "iteration 1000 / 1500: loss 2.141235\n",
      "iteration 1100 / 1500: loss 2.137052\n",
      "iteration 1200 / 1500: loss 2.123520\n",
      "iteration 1300 / 1500: loss 2.196068\n",
      "iteration 1400 / 1500: loss 2.187018\n",
      "1e-06 46415.8883361\n",
      "iteration 0 / 1500: loss 1433.934704\n",
      "iteration 100 / 1500: loss 2.288075\n",
      "iteration 200 / 1500: loss 2.172284\n",
      "iteration 300 / 1500: loss 2.090247\n",
      "iteration 400 / 1500: loss 2.227788\n",
      "iteration 500 / 1500: loss 2.168286\n",
      "iteration 600 / 1500: loss 2.171385\n",
      "iteration 700 / 1500: loss 2.144229\n",
      "iteration 800 / 1500: loss 2.205449\n",
      "iteration 900 / 1500: loss 2.240159\n",
      "iteration 1000 / 1500: loss 2.188455\n",
      "iteration 1100 / 1500: loss 2.117009\n",
      "iteration 1200 / 1500: loss 2.180707\n",
      "iteration 1300 / 1500: loss 2.169671\n",
      "iteration 1400 / 1500: loss 2.246453\n",
      "1e-06 100000.0\n",
      "iteration 0 / 1500: loss 3082.678590\n",
      "iteration 100 / 1500: loss 2.257288\n",
      "iteration 200 / 1500: loss 2.191194\n",
      "iteration 300 / 1500: loss 2.272304\n",
      "iteration 400 / 1500: loss 2.214431\n",
      "iteration 500 / 1500: loss 2.230604\n",
      "iteration 600 / 1500: loss 2.224077\n",
      "iteration 700 / 1500: loss 2.310379\n",
      "iteration 800 / 1500: loss 2.219466\n",
      "iteration 900 / 1500: loss 2.309946\n",
      "iteration 1000 / 1500: loss 2.251703\n",
      "iteration 1100 / 1500: loss 2.281208\n",
      "iteration 1200 / 1500: loss 2.235314\n",
      "iteration 1300 / 1500: loss 2.215340\n",
      "iteration 1400 / 1500: loss 2.274558\n",
      "1e-06 215443.469003\n",
      "iteration 0 / 1500: loss 6666.396357\n",
      "iteration 100 / 1500: loss 2.277670\n",
      "iteration 200 / 1500: loss 2.254914\n",
      "iteration 300 / 1500: loss 2.292892\n",
      "iteration 400 / 1500: loss 2.274173\n",
      "iteration 500 / 1500: loss 2.329638\n",
      "iteration 600 / 1500: loss 2.283702\n",
      "iteration 700 / 1500: loss 2.320279\n",
      "iteration 800 / 1500: loss 2.300681\n",
      "iteration 900 / 1500: loss 2.263594\n",
      "iteration 1000 / 1500: loss 2.303141\n",
      "iteration 1100 / 1500: loss 2.310460\n",
      "iteration 1200 / 1500: loss 2.274329\n",
      "iteration 1300 / 1500: loss 2.272303\n",
      "iteration 1400 / 1500: loss 2.280458\n",
      "1e-06 464158.883361\n",
      "iteration 0 / 1500: loss 14243.012115\n",
      "iteration 100 / 1500: loss 2.332211\n",
      "iteration 200 / 1500: loss 2.287326\n",
      "iteration 300 / 1500: loss 2.307411\n",
      "iteration 400 / 1500: loss 2.314069\n",
      "iteration 500 / 1500: loss 2.312585\n",
      "iteration 600 / 1500: loss 2.316913\n",
      "iteration 700 / 1500: loss 2.342537\n",
      "iteration 800 / 1500: loss 2.311214\n",
      "iteration 900 / 1500: loss 2.332066\n",
      "iteration 1000 / 1500: loss 2.348815\n",
      "iteration 1100 / 1500: loss 2.326451\n",
      "iteration 1200 / 1500: loss 2.348618\n",
      "iteration 1300 / 1500: loss 2.317919\n",
      "iteration 1400 / 1500: loss 2.296799\n",
      "1e-06 1000000.0\n",
      "iteration 0 / 1500: loss 30443.243616\n",
      "iteration 100 / 1500: loss 2.355407\n",
      "iteration 200 / 1500: loss 2.342185\n",
      "iteration 300 / 1500: loss 2.360025\n",
      "iteration 400 / 1500: loss 2.350989\n",
      "iteration 500 / 1500: loss 2.338706\n",
      "iteration 600 / 1500: loss 2.362404\n",
      "iteration 700 / 1500: loss 2.362496\n",
      "iteration 800 / 1500: loss 2.361277\n",
      "iteration 900 / 1500: loss 2.351795\n",
      "iteration 1000 / 1500: loss 2.338972\n",
      "iteration 1100 / 1500: loss 2.351611\n",
      "iteration 1200 / 1500: loss 2.365930\n",
      "iteration 1300 / 1500: loss 2.350592\n",
      "iteration 1400 / 1500: loss 2.350063\n",
      "3.16227766017e-06 100.0\n",
      "iteration 0 / 1500: loss 8.949755\n",
      "iteration 100 / 1500: loss 5.368684\n",
      "iteration 200 / 1500: loss 4.855944\n",
      "iteration 300 / 1500: loss 4.386484\n",
      "iteration 400 / 1500: loss 4.197833\n",
      "iteration 500 / 1500: loss 4.025758\n",
      "iteration 600 / 1500: loss 3.611720\n",
      "iteration 700 / 1500: loss 3.806151\n",
      "iteration 800 / 1500: loss 3.552877\n",
      "iteration 900 / 1500: loss 3.295472\n",
      "iteration 1000 / 1500: loss 3.351581\n",
      "iteration 1100 / 1500: loss 3.196475\n",
      "iteration 1200 / 1500: loss 3.074275\n",
      "iteration 1300 / 1500: loss 2.936370\n",
      "iteration 1400 / 1500: loss 2.804252\n",
      "3.16227766017e-06 215.443469003\n",
      "iteration 0 / 1500: loss 12.014750\n",
      "iteration 100 / 1500: loss 7.795410\n",
      "iteration 200 / 1500: loss 6.948377\n",
      "iteration 300 / 1500: loss 5.912451\n",
      "iteration 400 / 1500: loss 5.466612\n",
      "iteration 500 / 1500: loss 5.008786\n",
      "iteration 600 / 1500: loss 4.491059\n",
      "iteration 700 / 1500: loss 4.111364\n",
      "iteration 800 / 1500: loss 3.735040\n",
      "iteration 900 / 1500: loss 3.551834\n",
      "iteration 1000 / 1500: loss 3.388485\n",
      "iteration 1100 / 1500: loss 3.218988\n",
      "iteration 1200 / 1500: loss 2.859651\n",
      "iteration 1300 / 1500: loss 2.684132\n",
      "iteration 1400 / 1500: loss 2.706073\n",
      "3.16227766017e-06 464.158883361\n",
      "iteration 0 / 1500: loss 20.027881\n",
      "iteration 100 / 1500: loss 12.449609\n",
      "iteration 200 / 1500: loss 9.639247\n",
      "iteration 300 / 1500: loss 7.444703\n",
      "iteration 400 / 1500: loss 5.934342\n",
      "iteration 500 / 1500: loss 4.974710\n",
      "iteration 600 / 1500: loss 4.033351\n",
      "iteration 700 / 1500: loss 3.441495\n",
      "iteration 800 / 1500: loss 3.066359\n",
      "iteration 900 / 1500: loss 2.749444\n",
      "iteration 1000 / 1500: loss 2.406588\n",
      "iteration 1100 / 1500: loss 2.326226\n",
      "iteration 1200 / 1500: loss 2.148967\n",
      "iteration 1300 / 1500: loss 2.105313\n",
      "iteration 1400 / 1500: loss 1.879786\n",
      "3.16227766017e-06 1000.0\n",
      "iteration 0 / 1500: loss 36.531945\n",
      "iteration 100 / 1500: loss 18.096040\n",
      "iteration 200 / 1500: loss 10.234639\n",
      "iteration 300 / 1500: loss 6.318370\n",
      "iteration 400 / 1500: loss 4.093730\n",
      "iteration 500 / 1500: loss 3.090706\n",
      "iteration 600 / 1500: loss 2.526719\n",
      "iteration 700 / 1500: loss 2.279900\n",
      "iteration 800 / 1500: loss 1.984825\n",
      "iteration 900 / 1500: loss 1.966205\n",
      "iteration 1000 / 1500: loss 1.774519\n",
      "iteration 1100 / 1500: loss 2.017128\n",
      "iteration 1200 / 1500: loss 2.014073\n",
      "iteration 1300 / 1500: loss 1.833871\n",
      "iteration 1400 / 1500: loss 1.873688\n",
      "3.16227766017e-06 2154.43469003\n",
      "iteration 0 / 1500: loss 73.338805\n",
      "iteration 100 / 1500: loss 18.375682\n",
      "iteration 200 / 1500: loss 6.031855\n",
      "iteration 300 / 1500: loss 2.825176\n",
      "iteration 400 / 1500: loss 2.181719\n",
      "iteration 500 / 1500: loss 2.026052\n",
      "iteration 600 / 1500: loss 2.017388\n",
      "iteration 700 / 1500: loss 2.024261\n",
      "iteration 800 / 1500: loss 1.944085\n",
      "iteration 900 / 1500: loss 1.834598\n",
      "iteration 1000 / 1500: loss 1.906486\n",
      "iteration 1100 / 1500: loss 1.914492\n",
      "iteration 1200 / 1500: loss 2.057757\n",
      "iteration 1300 / 1500: loss 2.013053\n",
      "iteration 1400 / 1500: loss 1.760987\n",
      "3.16227766017e-06 4641.58883361\n",
      "iteration 0 / 1500: loss 150.521378\n",
      "iteration 100 / 1500: loss 9.365817\n",
      "iteration 200 / 1500: loss 2.320455\n",
      "iteration 300 / 1500: loss 2.013563\n",
      "iteration 400 / 1500: loss 2.126647\n",
      "iteration 500 / 1500: loss 2.028179\n",
      "iteration 600 / 1500: loss 1.996290\n",
      "iteration 700 / 1500: loss 2.027378\n",
      "iteration 800 / 1500: loss 2.075115\n",
      "iteration 900 / 1500: loss 2.034703\n",
      "iteration 1000 / 1500: loss 2.027072\n",
      "iteration 1100 / 1500: loss 2.099672\n",
      "iteration 1200 / 1500: loss 2.004104\n",
      "iteration 1300 / 1500: loss 2.055448\n",
      "iteration 1400 / 1500: loss 2.054559\n",
      "3.16227766017e-06 10000.0\n",
      "iteration 0 / 1500: loss 315.246744\n",
      "iteration 100 / 1500: loss 2.517024\n",
      "iteration 200 / 1500: loss 2.153399\n",
      "iteration 300 / 1500: loss 2.049994\n",
      "iteration 400 / 1500: loss 2.042961\n",
      "iteration 500 / 1500: loss 2.037124\n",
      "iteration 600 / 1500: loss 2.044327\n",
      "iteration 700 / 1500: loss 2.123302\n",
      "iteration 800 / 1500: loss 2.182076\n",
      "iteration 900 / 1500: loss 2.149979\n",
      "iteration 1000 / 1500: loss 2.095183\n",
      "iteration 1100 / 1500: loss 2.086643\n",
      "iteration 1200 / 1500: loss 2.142683\n",
      "iteration 1300 / 1500: loss 2.071663\n",
      "iteration 1400 / 1500: loss 2.134051\n",
      "3.16227766017e-06 21544.3469003\n",
      "iteration 0 / 1500: loss 680.745080\n",
      "iteration 100 / 1500: loss 2.210193\n",
      "iteration 200 / 1500: loss 2.184472\n",
      "iteration 300 / 1500: loss 2.188902\n",
      "iteration 400 / 1500: loss 2.206537\n",
      "iteration 500 / 1500: loss 2.175209\n",
      "iteration 600 / 1500: loss 2.231901\n",
      "iteration 700 / 1500: loss 2.113617\n",
      "iteration 800 / 1500: loss 2.180308\n",
      "iteration 900 / 1500: loss 2.146010\n",
      "iteration 1000 / 1500: loss 2.168403\n",
      "iteration 1100 / 1500: loss 2.174475\n",
      "iteration 1200 / 1500: loss 2.145721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1300 / 1500: loss 2.137727\n",
      "iteration 1400 / 1500: loss 2.294756\n",
      "3.16227766017e-06 46415.8883361\n",
      "iteration 0 / 1500: loss 1443.906605\n",
      "iteration 100 / 1500: loss 2.208930\n",
      "iteration 200 / 1500: loss 2.222653\n",
      "iteration 300 / 1500: loss 2.309563\n",
      "iteration 400 / 1500: loss 2.261817\n",
      "iteration 500 / 1500: loss 2.242330\n",
      "iteration 600 / 1500: loss 2.206190\n",
      "iteration 700 / 1500: loss 2.267209\n",
      "iteration 800 / 1500: loss 2.269163\n",
      "iteration 900 / 1500: loss 2.241685\n",
      "iteration 1000 / 1500: loss 2.246407\n",
      "iteration 1100 / 1500: loss 2.359205\n",
      "iteration 1200 / 1500: loss 2.260324\n",
      "iteration 1300 / 1500: loss 2.261943\n",
      "iteration 1400 / 1500: loss 2.205129\n",
      "3.16227766017e-06 100000.0\n",
      "iteration 0 / 1500: loss 3061.564496\n",
      "iteration 100 / 1500: loss 2.303155\n",
      "iteration 200 / 1500: loss 2.286007\n",
      "iteration 300 / 1500: loss 2.301007\n",
      "iteration 400 / 1500: loss 2.417692\n",
      "iteration 500 / 1500: loss 2.295036\n",
      "iteration 600 / 1500: loss 2.401379\n",
      "iteration 700 / 1500: loss 2.310637\n",
      "iteration 800 / 1500: loss 2.301888\n",
      "iteration 900 / 1500: loss 2.271595\n",
      "iteration 1000 / 1500: loss 2.289864\n",
      "iteration 1100 / 1500: loss 2.435276\n",
      "iteration 1200 / 1500: loss 2.396776\n",
      "iteration 1300 / 1500: loss 2.261013\n",
      "iteration 1400 / 1500: loss 2.375873\n",
      "3.16227766017e-06 215443.469003\n",
      "iteration 0 / 1500: loss 6701.567392\n",
      "iteration 100 / 1500: loss 2.598084\n",
      "iteration 200 / 1500: loss 2.525802\n",
      "iteration 300 / 1500: loss 2.486810\n",
      "iteration 400 / 1500: loss 2.518797\n",
      "iteration 500 / 1500: loss 2.538285\n",
      "iteration 600 / 1500: loss 2.462652\n",
      "iteration 700 / 1500: loss 2.421541\n",
      "iteration 800 / 1500: loss 2.617093\n",
      "iteration 900 / 1500: loss 2.487585\n",
      "iteration 1000 / 1500: loss 2.570125\n",
      "iteration 1100 / 1500: loss 2.743382\n",
      "iteration 1200 / 1500: loss 2.438377\n",
      "iteration 1300 / 1500: loss 2.450417\n",
      "iteration 1400 / 1500: loss 2.481494\n",
      "3.16227766017e-06 464158.883361\n",
      "iteration 0 / 1500: loss 14400.748666\n",
      "iteration 100 / 1500: loss 25.147203\n",
      "iteration 200 / 1500: loss 29.835758\n",
      "iteration 300 / 1500: loss 24.574202\n",
      "iteration 400 / 1500: loss 25.112349\n",
      "iteration 500 / 1500: loss 24.413853\n",
      "iteration 600 / 1500: loss 26.585259\n",
      "iteration 700 / 1500: loss 24.279894\n",
      "iteration 800 / 1500: loss 23.705160\n",
      "iteration 900 / 1500: loss 30.091311\n",
      "iteration 1000 / 1500: loss 26.681996\n",
      "iteration 1100 / 1500: loss 20.138146\n",
      "iteration 1200 / 1500: loss 24.751847\n",
      "iteration 1300 / 1500: loss 24.176591\n",
      "iteration 1400 / 1500: loss 27.523167\n",
      "3.16227766017e-06 1000000.0\n",
      "iteration 0 / 1500: loss 31153.483114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cs231n/classifiers/softmax.py:89: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.log(softmax_val[idx_train, y]).sum()\n",
      "cs231n/classifiers/softmax.py:88: RuntimeWarning: invalid value encountered in divide\n",
      "  softmax_val = classes / np.sum(classes, axis=1)[:, None]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "1e-05 100.0\n",
      "iteration 0 / 1500: loss 9.110276\n",
      "iteration 100 / 1500: loss 5.131049\n",
      "iteration 200 / 1500: loss 4.422302\n",
      "iteration 300 / 1500: loss 4.305910\n",
      "iteration 400 / 1500: loss 4.330189\n",
      "iteration 500 / 1500: loss 4.211967\n",
      "iteration 600 / 1500: loss 3.003769\n",
      "iteration 700 / 1500: loss 3.648379\n",
      "iteration 800 / 1500: loss 2.845863\n",
      "iteration 900 / 1500: loss 3.752576\n",
      "iteration 1000 / 1500: loss 3.473529\n",
      "iteration 1100 / 1500: loss 2.343813\n",
      "iteration 1200 / 1500: loss 3.311404\n",
      "iteration 1300 / 1500: loss 2.560036\n",
      "iteration 1400 / 1500: loss 2.742212\n",
      "1e-05 215.443469003\n",
      "iteration 0 / 1500: loss 12.799683\n",
      "iteration 100 / 1500: loss 7.451473\n",
      "iteration 200 / 1500: loss 4.800063\n",
      "iteration 300 / 1500: loss 4.342444\n",
      "iteration 400 / 1500: loss 4.029100\n",
      "iteration 500 / 1500: loss 3.690412\n",
      "iteration 600 / 1500: loss 4.227870\n",
      "iteration 700 / 1500: loss 2.689555\n",
      "iteration 800 / 1500: loss 2.860756\n",
      "iteration 900 / 1500: loss 3.318687\n",
      "iteration 1000 / 1500: loss 3.147207\n",
      "iteration 1100 / 1500: loss 2.975751\n",
      "iteration 1200 / 1500: loss 4.081377\n",
      "iteration 1300 / 1500: loss 2.650644\n",
      "iteration 1400 / 1500: loss 3.494064\n",
      "1e-05 464.158883361\n",
      "iteration 0 / 1500: loss 19.633203\n",
      "iteration 100 / 1500: loss 9.874537\n",
      "iteration 200 / 1500: loss 5.460578\n",
      "iteration 300 / 1500: loss 4.346190\n",
      "iteration 400 / 1500: loss 5.111987\n",
      "iteration 500 / 1500: loss 3.345130\n",
      "iteration 600 / 1500: loss 2.917701\n",
      "iteration 700 / 1500: loss 2.766521\n",
      "iteration 800 / 1500: loss 3.490198\n",
      "iteration 900 / 1500: loss 6.081366\n",
      "iteration 1000 / 1500: loss 2.849932\n",
      "iteration 1100 / 1500: loss 3.435386\n",
      "iteration 1200 / 1500: loss 3.339170\n",
      "iteration 1300 / 1500: loss 2.945253\n",
      "iteration 1400 / 1500: loss 3.006844\n",
      "1e-05 1000.0\n",
      "iteration 0 / 1500: loss 36.348748\n",
      "iteration 100 / 1500: loss 7.251928\n",
      "iteration 200 / 1500: loss 3.649379\n",
      "iteration 300 / 1500: loss 2.876460\n",
      "iteration 400 / 1500: loss 3.230007\n",
      "iteration 500 / 1500: loss 3.448602\n",
      "iteration 600 / 1500: loss 2.296638\n",
      "iteration 700 / 1500: loss 3.268553\n",
      "iteration 800 / 1500: loss 4.010703\n",
      "iteration 900 / 1500: loss 2.697503\n",
      "iteration 1000 / 1500: loss 3.106983\n",
      "iteration 1100 / 1500: loss 3.241194\n",
      "iteration 1200 / 1500: loss 2.850212\n",
      "iteration 1300 / 1500: loss 2.464330\n",
      "iteration 1400 / 1500: loss 3.270115\n",
      "1e-05 2154.43469003\n",
      "iteration 0 / 1500: loss 72.571421\n",
      "iteration 100 / 1500: loss 4.913087\n",
      "iteration 200 / 1500: loss 3.520096\n",
      "iteration 300 / 1500: loss 2.751504\n",
      "iteration 400 / 1500: loss 4.545666\n",
      "iteration 500 / 1500: loss 2.345000\n",
      "iteration 600 / 1500: loss 4.252436\n",
      "iteration 700 / 1500: loss 4.354795\n",
      "iteration 800 / 1500: loss 3.069082\n",
      "iteration 900 / 1500: loss 3.411167\n",
      "iteration 1000 / 1500: loss 2.870729\n",
      "iteration 1100 / 1500: loss 4.803290\n",
      "iteration 1200 / 1500: loss 3.228608\n",
      "iteration 1300 / 1500: loss 3.912208\n",
      "iteration 1400 / 1500: loss 4.259590\n",
      "1e-05 4641.58883361\n",
      "iteration 0 / 1500: loss 149.687768\n",
      "iteration 100 / 1500: loss 3.447672\n",
      "iteration 200 / 1500: loss 3.417086\n",
      "iteration 300 / 1500: loss 3.598717\n",
      "iteration 400 / 1500: loss 3.904145\n",
      "iteration 500 / 1500: loss 4.541281\n",
      "iteration 600 / 1500: loss 4.751192\n",
      "iteration 700 / 1500: loss 5.185350\n",
      "iteration 800 / 1500: loss 3.488159\n",
      "iteration 900 / 1500: loss 4.269151\n",
      "iteration 1000 / 1500: loss 3.595698\n",
      "iteration 1100 / 1500: loss 4.434254\n",
      "iteration 1200 / 1500: loss 3.695034\n",
      "iteration 1300 / 1500: loss 4.622137\n",
      "iteration 1400 / 1500: loss 5.184825\n",
      "1e-05 10000.0\n",
      "iteration 0 / 1500: loss 311.886780\n",
      "iteration 100 / 1500: loss 4.822362\n",
      "iteration 200 / 1500: loss 4.749751\n",
      "iteration 300 / 1500: loss 3.340950\n",
      "iteration 400 / 1500: loss 4.887514\n",
      "iteration 500 / 1500: loss 4.445111\n",
      "iteration 600 / 1500: loss 3.990698\n",
      "iteration 700 / 1500: loss 5.494746\n",
      "iteration 800 / 1500: loss 4.708248\n",
      "iteration 900 / 1500: loss 4.539053\n",
      "iteration 1000 / 1500: loss 4.113146\n",
      "iteration 1100 / 1500: loss 4.973383\n",
      "iteration 1200 / 1500: loss 6.011966\n",
      "iteration 1300 / 1500: loss 3.437580\n",
      "iteration 1400 / 1500: loss 6.733179\n",
      "1e-05 21544.3469003\n",
      "iteration 0 / 1500: loss 676.157574\n",
      "iteration 100 / 1500: loss 6.700590\n",
      "iteration 200 / 1500: loss 4.021230\n",
      "iteration 300 / 1500: loss 5.717076\n",
      "iteration 400 / 1500: loss 6.454136\n",
      "iteration 500 / 1500: loss 3.913627\n",
      "iteration 600 / 1500: loss 6.729267\n",
      "iteration 700 / 1500: loss 5.398024\n",
      "iteration 800 / 1500: loss 6.610318\n",
      "iteration 900 / 1500: loss 4.275381\n",
      "iteration 1000 / 1500: loss 6.218516\n",
      "iteration 1100 / 1500: loss 6.174481\n",
      "iteration 1200 / 1500: loss 4.488996\n",
      "iteration 1300 / 1500: loss 6.762833\n",
      "iteration 1400 / 1500: loss 7.639522\n",
      "1e-05 46415.8883361\n",
      "iteration 0 / 1500: loss 1440.881729\n",
      "iteration 100 / 1500: loss 8.189147\n",
      "iteration 200 / 1500: loss 8.673921\n",
      "iteration 300 / 1500: loss 7.105183\n",
      "iteration 400 / 1500: loss 9.897916\n",
      "iteration 500 / 1500: loss 9.050420\n",
      "iteration 600 / 1500: loss 11.550778\n",
      "iteration 700 / 1500: loss 8.758118\n",
      "iteration 800 / 1500: loss 6.713131\n",
      "iteration 900 / 1500: loss 8.512589\n",
      "iteration 1000 / 1500: loss 6.890592\n",
      "iteration 1100 / 1500: loss 7.803077\n",
      "iteration 1200 / 1500: loss 6.869098\n",
      "iteration 1300 / 1500: loss 13.068586\n",
      "iteration 1400 / 1500: loss 9.043443\n",
      "1e-05 100000.0\n",
      "iteration 0 / 1500: loss 3119.915730\n",
      "iteration 100 / 1500: loss 22.051690\n",
      "iteration 200 / 1500: loss 23.039196\n",
      "iteration 300 / 1500: loss 23.824438\n",
      "iteration 400 / 1500: loss 18.931054\n",
      "iteration 500 / 1500: loss 24.005777\n",
      "iteration 600 / 1500: loss 22.590990\n",
      "iteration 700 / 1500: loss 21.546617\n",
      "iteration 800 / 1500: loss 19.475756\n",
      "iteration 900 / 1500: loss 26.523016\n",
      "iteration 1000 / 1500: loss 26.521675\n",
      "iteration 1100 / 1500: loss 22.762922\n",
      "iteration 1200 / 1500: loss 24.007026\n",
      "iteration 1300 / 1500: loss 22.804848\n",
      "iteration 1400 / 1500: loss 25.051318\n",
      "1e-05 215443.469003\n",
      "iteration 0 / 1500: loss 6634.329864\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "1e-05 464158.883361\n",
      "iteration 0 / 1500: loss 14269.234578\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "1e-05 1000000.0\n",
      "iteration 0 / 1500: loss 30849.912756\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "lr 1.000000e-09 reg 1.000000e+02 train accuracy: 0.114939 val accuracy: 0.106000\n",
      "lr 1.000000e-09 reg 2.154435e+02 train accuracy: 0.105327 val accuracy: 0.111000\n",
      "lr 1.000000e-09 reg 4.641589e+02 train accuracy: 0.110367 val accuracy: 0.103000\n",
      "lr 1.000000e-09 reg 1.000000e+03 train accuracy: 0.126878 val accuracy: 0.153000\n",
      "lr 1.000000e-09 reg 2.154435e+03 train accuracy: 0.118980 val accuracy: 0.108000\n",
      "lr 1.000000e-09 reg 4.641589e+03 train accuracy: 0.085592 val accuracy: 0.092000\n",
      "lr 1.000000e-09 reg 1.000000e+04 train accuracy: 0.111408 val accuracy: 0.104000\n",
      "lr 1.000000e-09 reg 2.154435e+04 train accuracy: 0.091592 val accuracy: 0.098000\n",
      "lr 1.000000e-09 reg 4.641589e+04 train accuracy: 0.109694 val accuracy: 0.091000\n",
      "lr 1.000000e-09 reg 1.000000e+05 train accuracy: 0.098714 val accuracy: 0.099000\n",
      "lr 1.000000e-09 reg 2.154435e+05 train accuracy: 0.085020 val accuracy: 0.100000\n",
      "lr 1.000000e-09 reg 4.641589e+05 train accuracy: 0.117204 val accuracy: 0.124000\n",
      "lr 1.000000e-09 reg 1.000000e+06 train accuracy: 0.122449 val accuracy: 0.148000\n",
      "lr 3.162278e-09 reg 1.000000e+02 train accuracy: 0.120878 val accuracy: 0.114000\n",
      "lr 3.162278e-09 reg 2.154435e+02 train accuracy: 0.113959 val accuracy: 0.131000\n",
      "lr 3.162278e-09 reg 4.641589e+02 train accuracy: 0.131224 val accuracy: 0.141000\n",
      "lr 3.162278e-09 reg 1.000000e+03 train accuracy: 0.134367 val accuracy: 0.140000\n",
      "lr 3.162278e-09 reg 2.154435e+03 train accuracy: 0.104592 val accuracy: 0.105000\n",
      "lr 3.162278e-09 reg 4.641589e+03 train accuracy: 0.106429 val accuracy: 0.106000\n",
      "lr 3.162278e-09 reg 1.000000e+04 train accuracy: 0.113510 val accuracy: 0.092000\n",
      "lr 3.162278e-09 reg 2.154435e+04 train accuracy: 0.111469 val accuracy: 0.125000\n",
      "lr 3.162278e-09 reg 4.641589e+04 train accuracy: 0.125980 val accuracy: 0.131000\n",
      "lr 3.162278e-09 reg 1.000000e+05 train accuracy: 0.132918 val accuracy: 0.121000\n",
      "lr 3.162278e-09 reg 2.154435e+05 train accuracy: 0.146653 val accuracy: 0.141000\n",
      "lr 3.162278e-09 reg 4.641589e+05 train accuracy: 0.168878 val accuracy: 0.181000\n",
      "lr 3.162278e-09 reg 1.000000e+06 train accuracy: 0.250143 val accuracy: 0.262000\n",
      "lr 1.000000e-08 reg 1.000000e+02 train accuracy: 0.154388 val accuracy: 0.147000\n",
      "lr 1.000000e-08 reg 2.154435e+02 train accuracy: 0.148857 val accuracy: 0.157000\n",
      "lr 1.000000e-08 reg 4.641589e+02 train accuracy: 0.162490 val accuracy: 0.199000\n",
      "lr 1.000000e-08 reg 1.000000e+03 train accuracy: 0.153571 val accuracy: 0.145000\n",
      "lr 1.000000e-08 reg 2.154435e+03 train accuracy: 0.156286 val accuracy: 0.161000\n",
      "lr 1.000000e-08 reg 4.641589e+03 train accuracy: 0.169388 val accuracy: 0.164000\n",
      "lr 1.000000e-08 reg 1.000000e+04 train accuracy: 0.142163 val accuracy: 0.153000\n",
      "lr 1.000000e-08 reg 2.154435e+04 train accuracy: 0.177469 val accuracy: 0.156000\n",
      "lr 1.000000e-08 reg 4.641589e+04 train accuracy: 0.170306 val accuracy: 0.181000\n",
      "lr 1.000000e-08 reg 1.000000e+05 train accuracy: 0.206000 val accuracy: 0.213000\n",
      "lr 1.000000e-08 reg 2.154435e+05 train accuracy: 0.269347 val accuracy: 0.286000\n",
      "lr 1.000000e-08 reg 4.641589e+05 train accuracy: 0.271245 val accuracy: 0.279000\n",
      "lr 1.000000e-08 reg 1.000000e+06 train accuracy: 0.252224 val accuracy: 0.261000\n",
      "lr 3.162278e-08 reg 1.000000e+02 train accuracy: 0.197306 val accuracy: 0.189000\n",
      "lr 3.162278e-08 reg 2.154435e+02 train accuracy: 0.188510 val accuracy: 0.189000\n",
      "lr 3.162278e-08 reg 4.641589e+02 train accuracy: 0.214714 val accuracy: 0.238000\n",
      "lr 3.162278e-08 reg 1.000000e+03 train accuracy: 0.198245 val accuracy: 0.194000\n",
      "lr 3.162278e-08 reg 2.154435e+03 train accuracy: 0.201061 val accuracy: 0.201000\n",
      "lr 3.162278e-08 reg 4.641589e+03 train accuracy: 0.205837 val accuracy: 0.215000\n",
      "lr 3.162278e-08 reg 1.000000e+04 train accuracy: 0.222082 val accuracy: 0.224000\n",
      "lr 3.162278e-08 reg 2.154435e+04 train accuracy: 0.250143 val accuracy: 0.246000\n",
      "lr 3.162278e-08 reg 4.641589e+04 train accuracy: 0.299980 val accuracy: 0.319000\n",
      "lr 3.162278e-08 reg 1.000000e+05 train accuracy: 0.306143 val accuracy: 0.319000\n",
      "lr 3.162278e-08 reg 2.154435e+05 train accuracy: 0.290571 val accuracy: 0.304000\n",
      "lr 3.162278e-08 reg 4.641589e+05 train accuracy: 0.265531 val accuracy: 0.272000\n",
      "lr 3.162278e-08 reg 1.000000e+06 train accuracy: 0.255184 val accuracy: 0.270000\n",
      "lr 1.000000e-07 reg 1.000000e+02 train accuracy: 0.242898 val accuracy: 0.235000\n",
      "lr 1.000000e-07 reg 2.154435e+02 train accuracy: 0.247061 val accuracy: 0.264000\n",
      "lr 1.000000e-07 reg 4.641589e+02 train accuracy: 0.254020 val accuracy: 0.266000\n",
      "lr 1.000000e-07 reg 1.000000e+03 train accuracy: 0.255041 val accuracy: 0.268000\n",
      "lr 1.000000e-07 reg 2.154435e+03 train accuracy: 0.264306 val accuracy: 0.265000\n",
      "lr 1.000000e-07 reg 4.641589e+03 train accuracy: 0.282898 val accuracy: 0.300000\n",
      "lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.335408 val accuracy: 0.340000\n",
      "lr 1.000000e-07 reg 2.154435e+04 train accuracy: 0.350918 val accuracy: 0.367000\n",
      "lr 1.000000e-07 reg 4.641589e+04 train accuracy: 0.330714 val accuracy: 0.345000\n",
      "lr 1.000000e-07 reg 1.000000e+05 train accuracy: 0.314245 val accuracy: 0.326000\n",
      "lr 1.000000e-07 reg 2.154435e+05 train accuracy: 0.278755 val accuracy: 0.290000\n",
      "lr 1.000000e-07 reg 4.641589e+05 train accuracy: 0.271633 val accuracy: 0.282000\n",
      "lr 1.000000e-07 reg 1.000000e+06 train accuracy: 0.261306 val accuracy: 0.270000\n",
      "lr 3.162278e-07 reg 1.000000e+02 train accuracy: 0.303857 val accuracy: 0.315000\n",
      "lr 3.162278e-07 reg 2.154435e+02 train accuracy: 0.302571 val accuracy: 0.307000\n",
      "lr 3.162278e-07 reg 4.641589e+02 train accuracy: 0.312959 val accuracy: 0.322000\n",
      "lr 3.162278e-07 reg 1.000000e+03 train accuracy: 0.332122 val accuracy: 0.322000\n",
      "lr 3.162278e-07 reg 2.154435e+03 train accuracy: 0.358816 val accuracy: 0.382000\n",
      "lr 3.162278e-07 reg 4.641589e+03 train accuracy: 0.382857 val accuracy: 0.400000\n",
      "lr 3.162278e-07 reg 1.000000e+04 train accuracy: 0.369429 val accuracy: 0.387000\n",
      "lr 3.162278e-07 reg 2.154435e+04 train accuracy: 0.354898 val accuracy: 0.370000\n",
      "lr 3.162278e-07 reg 4.641589e+04 train accuracy: 0.334510 val accuracy: 0.342000\n",
      "lr 3.162278e-07 reg 1.000000e+05 train accuracy: 0.301367 val accuracy: 0.312000\n",
      "lr 3.162278e-07 reg 2.154435e+05 train accuracy: 0.269633 val accuracy: 0.279000\n",
      "lr 3.162278e-07 reg 4.641589e+05 train accuracy: 0.267327 val accuracy: 0.270000\n",
      "lr 3.162278e-07 reg 1.000000e+06 train accuracy: 0.251082 val accuracy: 0.269000\n",
      "lr 1.000000e-06 reg 1.000000e+02 train accuracy: 0.358755 val accuracy: 0.351000\n",
      "lr 1.000000e-06 reg 2.154435e+02 train accuracy: 0.363102 val accuracy: 0.351000\n",
      "lr 1.000000e-06 reg 4.641589e+02 train accuracy: 0.387367 val accuracy: 0.377000\n",
      "lr 1.000000e-06 reg 1.000000e+03 train accuracy: 0.398408 val accuracy: 0.391000\n",
      "lr 1.000000e-06 reg 2.154435e+03 train accuracy: 0.393224 val accuracy: 0.386000\n",
      "lr 1.000000e-06 reg 4.641589e+03 train accuracy: 0.384714 val accuracy: 0.388000\n",
      "lr 1.000000e-06 reg 1.000000e+04 train accuracy: 0.367245 val accuracy: 0.375000\n",
      "lr 1.000000e-06 reg 2.154435e+04 train accuracy: 0.348959 val accuracy: 0.361000\n",
      "lr 1.000000e-06 reg 4.641589e+04 train accuracy: 0.317000 val accuracy: 0.325000\n",
      "lr 1.000000e-06 reg 1.000000e+05 train accuracy: 0.302327 val accuracy: 0.311000\n",
      "lr 1.000000e-06 reg 2.154435e+05 train accuracy: 0.259776 val accuracy: 0.275000\n",
      "lr 1.000000e-06 reg 4.641589e+05 train accuracy: 0.231082 val accuracy: 0.239000\n",
      "lr 1.000000e-06 reg 1.000000e+06 train accuracy: 0.193857 val accuracy: 0.188000\n",
      "lr 3.162278e-06 reg 1.000000e+02 train accuracy: 0.401816 val accuracy: 0.369000\n",
      "lr 3.162278e-06 reg 2.154435e+02 train accuracy: 0.402959 val accuracy: 0.397000\n",
      "lr 3.162278e-06 reg 4.641589e+02 train accuracy: 0.412612 val accuracy: 0.381000\n",
      "lr 3.162278e-06 reg 1.000000e+03 train accuracy: 0.389061 val accuracy: 0.378000\n",
      "lr 3.162278e-06 reg 2.154435e+03 train accuracy: 0.384571 val accuracy: 0.370000\n",
      "lr 3.162278e-06 reg 4.641589e+03 train accuracy: 0.360347 val accuracy: 0.379000\n",
      "lr 3.162278e-06 reg 1.000000e+04 train accuracy: 0.354816 val accuracy: 0.360000\n",
      "lr 3.162278e-06 reg 2.154435e+04 train accuracy: 0.321918 val accuracy: 0.337000\n",
      "lr 3.162278e-06 reg 4.641589e+04 train accuracy: 0.289306 val accuracy: 0.320000\n",
      "lr 3.162278e-06 reg 1.000000e+05 train accuracy: 0.227694 val accuracy: 0.243000\n",
      "lr 3.162278e-06 reg 2.154435e+05 train accuracy: 0.188408 val accuracy: 0.201000\n",
      "lr 3.162278e-06 reg 4.641589e+05 train accuracy: 0.140122 val accuracy: 0.144000\n",
      "lr 3.162278e-06 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 1.000000e+02 train accuracy: 0.353776 val accuracy: 0.329000\n",
      "lr 1.000000e-05 reg 2.154435e+02 train accuracy: 0.330000 val accuracy: 0.333000\n",
      "lr 1.000000e-05 reg 4.641589e+02 train accuracy: 0.291163 val accuracy: 0.280000\n",
      "lr 1.000000e-05 reg 1.000000e+03 train accuracy: 0.229816 val accuracy: 0.222000\n",
      "lr 1.000000e-05 reg 2.154435e+03 train accuracy: 0.257551 val accuracy: 0.281000\n",
      "lr 1.000000e-05 reg 4.641589e+03 train accuracy: 0.215000 val accuracy: 0.221000\n",
      "lr 1.000000e-05 reg 1.000000e+04 train accuracy: 0.167735 val accuracy: 0.174000\n",
      "lr 1.000000e-05 reg 2.154435e+04 train accuracy: 0.190163 val accuracy: 0.208000\n",
      "lr 1.000000e-05 reg 4.641589e+04 train accuracy: 0.179143 val accuracy: 0.184000\n",
      "lr 1.000000e-05 reg 1.000000e+05 train accuracy: 0.100082 val accuracy: 0.104000\n",
      "lr 1.000000e-05 reg 2.154435e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 4.641589e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e-05 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved during cross-validation: 0.400000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "import itertools as it\n",
    "results = {}\n",
    "\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "learning_rates = np.logspace(-9, -5, num=9)\n",
    "regularization_strengths = np.logspace(2, 6, num=13)\n",
    "combs = it.product(learning_rates, regularization_strengths)\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "for lr, reg in combs:\n",
    "    print(lr, reg)\n",
    "    softmax = Softmax()\n",
    "    loss_hist = softmax.train(X_train, y_train, learning_rate=lr, reg=reg,\n",
    "                              num_iters=1500, verbose=True)\n",
    "    y_train_pred = softmax.predict(X_train)\n",
    "    y_val_pred = softmax.predict(X_val)\n",
    "    \n",
    "    train_acc = np.mean(y_train == y_train_pred)\n",
    "    val_acc = (np.mean(y_val == y_val_pred))\n",
    "\n",
    "    results[(lr, reg)] = (train_acc, val_acc)\n",
    "    if best_val < val_acc:\n",
    "        best_val = val_acc\n",
    "        best_softmax = softmax\n",
    "        best_lr = lr\n",
    "        best_reg = reg\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T14:56:06.060834+03:00",
     "start_time": "2017-12-08T14:56:06.026125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.380000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-08T14:56:07.177031+03:00",
     "start_time": "2017-12-08T14:56:06.064171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF8CAYAAADrUz6WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnMruJ31xAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f61fc5bfc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
